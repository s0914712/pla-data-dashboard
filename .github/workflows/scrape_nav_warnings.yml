# .github/workflows/scrape_nav_warnings.yml
name: Scrape MSA Military Exercise Warnings

on:
  schedule:
    # æ¯å¤© UTC 02:00 (åŒ—äº¬æ™‚é–“ 10:00) åŸ·è¡Œ
    - cron: '0 2 * * *'
  
  workflow_dispatch:  # å…è¨±æ‰‹å‹•è§¸ç™¼
  
  push:
    branches:
      - main
    paths:
      - 'scripts/scrapers/NavigationWarning_scraper.py'
      - '.github/workflows/scrape_nav_warnings.yml'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: ğŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 pandas lxml
    
    - name: ğŸ¯ Run Military Exercise Scraper
      run: |
        python -c "
        import sys
        import json
        import pandas as pd
        from pathlib import Path
        from datetime import datetime
        from collections import Counter
        
        sys.path.insert(0, 'scripts')
        from scrapers.NavigationWarning_scraper import NavigationWarningScraper
        
        print('ğŸ¯ é–‹å§‹çˆ¬å–è»äº‹æ¼”ç¿’ç›¸é—œèˆªè¡Œè­¦å‘Š...')
        
        with NavigationWarningScraper(delay=1.0) as scraper:
            warnings = scraper.run(max_pages=50, days_back=365)
        
        if not warnings:
            print('âš ï¸  æœªç²å–åˆ°è»äº‹æ¼”ç¿’æ•¸æ“š')
            # ä¸é€€å‡ºï¼Œå‰µå»ºç©ºæ–‡ä»¶
            warnings = []
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_dir = Path('data/navigation_warnings')
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ CSV
        if warnings:
            df = pd.DataFrame(warnings)
            csv_path = output_dir / 'military_exercises.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f'ğŸ’¾ å·²ä¿å­˜: {csv_path} ({len(df)} æ¢è¨˜éŒ„)')
        else:
            csv_path = output_dir / 'military_exercises.csv'
            pd.DataFrame(columns=['date', 'title', 'msa', 'matched_keywords', 'article_id', 'url', 'scraped_at']).to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f'ğŸ’¾ å·²ä¿å­˜ç©ºæ–‡ä»¶: {csv_path}')
        
        # ä¿å­˜ JSON
        json_path = output_dir / 'military_exercises.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(warnings, f, ensure_ascii=False, indent=2)
        print(f'ğŸ’¾ å·²ä¿å­˜: {json_path}')
        
        # ç”Ÿæˆçµ±è¨ˆ
        if warnings:
            all_keywords = []
            for w in warnings:
                if w.get('matched_keywords'):
                    all_keywords.extend(w['matched_keywords'].split(','))
            
            stats = {
                'update_time': datetime.now().isoformat(),
                'total_warnings': len(warnings),
                'date_range': {
                    'earliest': min(w['date'] for w in warnings),
                    'latest': max(w['date'] for w in warnings)
                },
                'by_msa': dict(Counter(w['msa'] for w in warnings)),
                'by_keyword': dict(Counter(all_keywords).most_common(20))
            }
        else:
            stats = {
                'update_time': datetime.now().isoformat(),
                'total_warnings': 0,
                'date_range': {'earliest': None, 'latest': None},
                'by_msa': {},
                'by_keyword': {}
            }
        
        stats_path = output_dir / 'statistics.json'
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f'ğŸ’¾ å·²ä¿å­˜: {stats_path}')
        
        # ç”Ÿæˆ README
        readme_content = f'''# ä¸­åœ‹æµ·äº‹å±€è»äº‹æ¼”ç¿’èˆªè¡Œè­¦å‘Šæ•¸æ“š

## ğŸ“Š æ•¸æ“šæ¦‚è¦½

- **æ›´æ–°æ™‚é–“**: {stats['update_time']}
- **ç¸½è­¦å‘Šæ•¸**: {stats['total_warnings']} æ¢
- **æ—¥æœŸç¯„åœ**: {stats['date_range']['earliest'] or 'N/A'} è‡³ {stats['date_range']['latest'] or 'N/A'}

## ğŸ¯ æ•¸æ“šç¯„åœ

æœ¬æ•¸æ“šé›†**åƒ…åŒ…å«è»äº‹æ¼”ç¿’ç›¸é—œ**çš„èˆªè¡Œè­¦å‘Šï¼Œé€šéä»¥ä¸‹é—œéµå­—ç¯©é¸ï¼š

### æ ¸å¿ƒé—œéµå­—
- å†›äº‹/è»äº‹ã€æ¼”ä¹ /æ¼”ç¿’ã€å®å¼¹/å¯¦å½ˆ
- ç«ç‚®å°„å‡»ã€å°„å‡»è®­ç»ƒã€ç¦èˆªã€ç¦æ­¢é©¶å…¥
- ç«ç®­å‘å°„ã€ç«ç®­æ®‹éª¸ã€MILITARYã€EXERCISE

### æ“´å±•é—œéµå­—
- å†›æ¼”ã€æ¼”è®­ã€è”åˆæ¼”ç»ƒã€å®æˆ˜åŒ–è®­ç»ƒ
- å¯¼å¼¹è¯•å°„ã€æ­¦å™¨è¯•éªŒã€æµ·ä¸Šå®å¼¹
- æˆ˜å¤‡å·¡èˆªã€å†›äº‹ç¦åŒºã€é¶åœº

## ğŸ¢ å„æµ·äº‹å±€çµ±è¨ˆ

| æµ·äº‹å±€ | è­¦å‘Šæ•¸é‡ |
|--------|----------|
'''
        if stats['by_msa']:
            for msa, count in sorted(stats['by_msa'].items(), key=lambda x: x[1], reverse=True):
                readme_content += f'| {msa} | {count} |\n'
        else:
            readme_content += '| æš«ç„¡æ•¸æ“š | 0 |\n'
        
        readme_content += f'''
## ğŸ” é—œéµå­—çµ±è¨ˆ (Top 20)

| é—œéµå­— | å‡ºç¾æ¬¡æ•¸ |
|--------|----------|
'''
        if stats['by_keyword']:
            for keyword, count in list(stats['by_keyword'].items())[:20]:
                readme_content += f'| {keyword} | {count} |\n'
        else:
            readme_content += '| æš«ç„¡æ•¸æ“š | 0 |\n'
        
        readme_content += f'''
## ğŸ“‚ æ•¸æ“šæ–‡ä»¶

- \`military_exercises.csv\` - è»æ¼”è­¦å‘Šï¼ˆCSVæ ¼å¼ï¼‰
- \`military_exercises.json\` - è»æ¼”è­¦å‘Šï¼ˆJSONæ ¼å¼ï¼‰
- \`statistics.json\` - çµ±è¨ˆä¿¡æ¯

## ğŸ“– æ•¸æ“šå­—æ®µèªªæ˜

| å­—æ®µ | èªªæ˜ | ç¤ºä¾‹ |
|------|------|------|
| \`date\` | ç™¼å¸ƒæ—¥æœŸ | 2024-01-15 |
| \`title\` | èˆªè¡Œè­¦å‘Šæ¨™é¡Œ | å†›äº‹æ¼”ä¹ â€”é—½èˆªè­¦123/24 |
| \`msa\` | ç™¼å¸ƒæµ·äº‹å±€ | ç¦å»ºæµ·äº‹å±€ |
| \`matched_keywords\` | åŒ¹é…çš„è»äº‹é—œéµå­— | å†›äº‹,æ¼”ä¹  |
| \`article_id\` | æ–‡ç« ID | abc123... |
| \`url\` | è©³ç´°ä¿¡æ¯éˆæ¥ | https://... |
| \`scraped_at\` | æŠ“å–æ™‚é–“ | 2024-01-16T10:00:00 |

## âš–ï¸ æ•¸æ“šä¾†æº

æ•¸æ“šä¾†æºï¼š[ä¸­è¯äººæ°‘å…±å’Œåœ‹æµ·äº‹å±€](https://www.msa.gov.cn)

## ğŸ”„ æ›´æ–°é »ç‡

æ¯æ—¥è‡ªå‹•æ›´æ–°ï¼ˆåŒ—äº¬æ™‚é–“ 10:00ï¼‰

---

*æœ€å¾Œæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
'''
        
        readme_path = output_dir / 'README.md'
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        print(f'ğŸ’¾ å·²ä¿å­˜: {readme_path}')
        
        print(f'\nâœ… ä»»å‹™å®Œæˆï¼å…± {len(warnings)} æ¢è»äº‹æ¼”ç¿’è­¦å‘Š')
        "
    
    - name: ğŸ“Š Display statistics
      run: |
        if [ -f data/navigation_warnings/statistics.json ]; then
          echo "ğŸ“ˆ çµ±è¨ˆä¿¡æ¯:"
          cat data/navigation_warnings/statistics.json | python -m json.tool
        fi
    
    - name: ğŸ’¾ Commit and push changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        git add data/navigation_warnings/
        
        if git diff --staged --quiet; then
          echo "ğŸ“­ No changes to commit"
        else
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
          git commit -m "ğŸ¯ Auto-update: Military Exercise Warnings - ${TIMESTAMP}"
          git push
          echo "âœ… Changes committed and pushed"
        fi
    
    - name: ğŸ“¤ Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: military-exercise-warnings
        path: |
          data/navigation_warnings/*.csv
          data/navigation_warnings/*.json
          data/navigation_warnings/README.md
        retention-days: 90
