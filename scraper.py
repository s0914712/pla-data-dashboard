from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from datetime import datetime
import os

# ==================== è¨­å®šå€ ====================
CSV_FILE = 'data/JapanandBattleship.csv'
base_url = "https://www.mnd.gov.tw/news/plaactlist"
total_pages = 4
start_page = 1
# =================================================

def init_driver():
    chrome_options = Options()
    
    # [GitHub Actions ä¿®æ­£é—œéµ]
    # åœ¨ GitHub Actions å¿…é ˆä½¿ç”¨ Headless æ¨¡å¼ (å› ç‚ºæ²’æœ‰è¢å¹•)ã€‚
    # ä½†èˆŠç‰ˆ --headless å®¹æ˜“è¢«æ“‹ï¼Œå¿…é ˆä½¿ç”¨æ–°ç‰ˆ "--headless=new" æ‰èƒ½é¨™éé˜²ç«ç‰†ã€‚
    chrome_options.add_argument('--headless=new')
    
    # CI/CD ç’°å¢ƒæ¨™æº–è¨­å®š
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--window-size=1920,1080') # å¼·åˆ¶è¨­å®šè¦–çª—å¤§å°ï¼Œé¿å… RWD éš±è—å…ƒç´ 
    
    # åçˆ¬èŸ²å½è£è¨­å®š
    chrome_options.add_argument('--disable-blink-features=AutomationControlled') 
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"]) 
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument('--lang=zh-TW') # æ¨¡æ“¬ç¹é«”ä¸­æ–‡ç’°å¢ƒ
    
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36')
    
    # ä½¿ç”¨ webdriver_manager è‡ªå‹•ç®¡ç†é©…å‹• (è‹¥å ±éŒ¯å¯æ”¹å›ç›´æ¥å‘¼å« webdriver.Chrome())
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
    except:
        # å‚™ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨ç³»çµ±è·¯å¾‘çš„ chromedriver
        driver = webdriver.Chrome(options=chrome_options)
        
    return driver

def extract_numbers_from_text(text):
    """å¾æ–‡æœ¬ä¸­æå–å…±æ©Ÿå…±è‰¦æ•¸é‡"""
    aircraft = 0
    vessel = 0

    aircraft_match = re.search(r'å…±æ©Ÿ\s*(\d+)\s*æ¶æ¬¡', text)
    if aircraft_match:
        aircraft = int(aircraft_match.group(1))

    vessel_match = re.search(r'å…±è‰¦\s*(\d+)\s*è‰˜', text)
    if vessel_match:
        vessel = int(vessel_match.group(1))

    return aircraft, vessel

def get_latest_date_from_csv():
    """å¾ CSV è®€å–æœ€æ–°æ—¥æœŸ"""
    try:
        if not os.path.exists(CSV_FILE):
            return None

        df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')

        if df.empty or 'date' not in df.columns:
            return None

        # ç¢ºä¿æ—¥æœŸæ ¼å¼æ­£ç¢ºè®€å–
        dates = pd.to_datetime(df['date'], format='%Y/%m/%d', errors='coerce')
        latest_date = dates.max()

        if pd.isna(latest_date):
            return None

        return latest_date

    except Exception as e:
        print(f"è®€å– CSV æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def save_to_csv(new_data):
    """å°‡æ–°è³‡æ–™é™„åŠ åˆ° CSV"""
    if not new_data:
        print("æ²’æœ‰æ–°è³‡æ–™éœ€è¦å¯«å…¥")
        return

    os.makedirs(os.path.dirname(CSV_FILE), exist_ok=True)

    if os.path.exists(CSV_FILE):
        df_existing = pd.read_csv(CSV_FILE, encoding='utf-8-sig')
    else:
        df_existing = pd.DataFrame(columns=['date', 'pla_aircraft_sorties', 'plan_vessel_sorties'])

    df_new = pd.DataFrame(new_data, columns=['date', 'pla_aircraft_sorties', 'plan_vessel_sorties'])
    df_combined = pd.concat([df_existing, df_new], ignore_index=True)

    # çµ±ä¸€æ—¥æœŸæ ¼å¼ä¸¦å»é‡
    df_combined['date'] = pd.to_datetime(df_combined['date'], format='%Y/%m/%d')
    df_combined = df_combined.sort_values('date')
    df_combined['date'] = df_combined['date'].dt.strftime('%Y/%m/%d')
    df_combined = df_combined.drop_duplicates(subset=['date'], keep='last')

    df_combined.to_csv(CSV_FILE, index=False, encoding='utf-8-sig')
    print(f"æˆåŠŸå¯«å…¥ {len(new_data)} ç­†è³‡æ–™åˆ° {CSV_FILE}")

def main():
    print(f"\n{'='*60}")
    print("é–‹å§‹çˆ¬å–åœ‹é˜²éƒ¨è³‡æ–™...")
    print(f"{'='*60}\n")

    latest_date = get_latest_date_from_csv()
    if latest_date:
        print(f"ğŸ“… CSV æœ€æ–°æ—¥æœŸ: {latest_date.strftime('%Y/%m/%d')}")
    else:
        print(f"ç„¡ç¾æœ‰è³‡æ–™ï¼Œå°‡çˆ¬å–æ‰€æœ‰è³‡æ–™")
        latest_date = datetime.min

    all_data = []
    processed_urls = set()

    driver = init_driver()
    print("âœ“ ç€è¦½å™¨å•Ÿå‹•æˆåŠŸ\n")

    try:
        for page in range(start_page, total_pages + 1):
            try:
                page_url = base_url if page == 1 else f"{base_url}&Page={page}"

                print(f"ğŸ“„ ç¬¬ {page} é : {page_url}")
                driver.get(page_url)
                
                # ç­‰å¾…åˆ—è¡¨è®€å–
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(3) # çµ¦äºˆé¡å¤–ç·©è¡æ™‚é–“

                soup = BeautifulSoup(driver.page_source, "html.parser")

                # æ–¹æ³•1: BS4 æ‰¾ plaact é€£çµ
                links = soup.find_all("a", href=re.compile(r'/news/plaact/\d+'))

                # æ–¹æ³•2: Selenium è£œå¼· (é‡å°å‹•æ…‹åŠ è¼‰)
                if not links:
                    selenium_links = driver.find_elements(By.TAG_NAME, "a")
                    links = []
                    for link in selenium_links:
                        try:
                            href = link.get_attribute("href")
                            text = link.text
                            if href and "plaact" in href and ("ä¸­å…±" in text or "å‹•æ…‹" in text):
                                links.append({'href': href, 'text': text})
                        except:
                            continue
                else:
                    links = [{'href': f"https://www.mnd.gov.tw{link.get('href')}", 
                              'text': link.get_text(strip=True)} for link in links]

                print(f"  æ‰¾åˆ° {len(links)} å€‹ plaact é€£çµ")

                for idx, link_info in enumerate(links, 1):
                    try:
                        if isinstance(link_info, dict):
                            detail_url = link_info['href']
                        else:
                            detail_url = f"https://www.mnd.gov.tw{link_info.get('href')}"

                        if not detail_url.startswith('http'):
                            detail_url = f"https://www.mnd.gov.tw{detail_url}"

                        if detail_url in processed_urls:
                            continue
                        processed_urls.add(detail_url)

                        # print(f"  [{idx:2d}] è®€å–ä¸­...", end=" ") 

                        # è¨ªå•è©³ç´°é é¢
                        driver.get(detail_url)
                        WebDriverWait(driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        time.sleep(2) # é—œéµï¼šç­‰å¾…å…§æ–‡æ¸²æŸ“

                        # ç²å–é é¢å…§å®¹
                        detail_soup = BeautifulSoup(driver.page_source, "html.parser")
                        
                        # å¢åŠ é˜²å‘†ï¼šå¦‚æœ body ç‚º None
                        if not detail_soup.body:
                            print(f"  [{idx:2d}] âš ï¸ æŠ“å–åˆ°çš„é é¢æ²’æœ‰ bodyï¼Œå¯èƒ½è¼‰å…¥å¤±æ•—")
                            continue
                            
                        body_text = detail_soup.body.get_text(separator="\n", strip=True)

                        # [ä¿®æ­£é» 3] å„ªåŒ–æ—¥æœŸæå–æ­£å‰‡è¡¨é”å¼ï¼Œå®¹è¨±ç©ºæ ¼
                        date = None
                        # æ ¼å¼ï¼šä¸­è¯æ°‘åœ‹ 114 å¹´ 2 æœˆ 13 æ—¥ (å…è¨±ä¸­é–“æœ‰ç©ºç™½)
                        date_match = re.search(r'ä¸­è¯æ°‘åœ‹\s*(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥', body_text)
                        
                        if date_match:
                            roc_year = int(date_match.group(1))
                            month = date_match.group(2).zfill(2)
                            day = date_match.group(3).zfill(2)
                            west_year = roc_year + 1911
                            date = f"{west_year}/{month}/{day}"

                        # å‚™ç”¨æ—¥æœŸæ ¼å¼ (114å¹´2æœˆ13æ—¥)
                        if not date:
                            alt_match = re.search(r'(\d{2,3})\s*å¹´\s*(\d{1,2})\s*æœˆ\s*(\d{1,2})\s*æ—¥', body_text)
                            if alt_match:
                                roc_year = int(alt_match.group(1))
                                month = alt_match.group(2).zfill(2)
                                day = alt_match.group(3).zfill(2)
                                west_year = roc_year + 1911
                                date = f"{west_year}/{month}/{day}"

                        if not date:
                            print(f"  [{idx:2d}] âš ï¸ æ‰¾ä¸åˆ°æ—¥æœŸï¼Œè·³é")
                            
                            # ==================== DEBUG å€åŸŸ ====================
                            print(f"    ğŸ” [DEBUG] ç¶²é æ¨™é¡Œ: {driver.title}")
                            print(f"    ğŸ” [DEBUG] ç•¶å‰ç¶²å€: {driver.current_url}")
                            # é è¦½æŠ“åˆ°çš„æ–‡å­—ï¼Œç¢ºèªæ˜¯å¦è¢«æ“‹
                            preview_text = body_text[:200].replace('\n', ' ') if body_text else "ç„¡å…§å®¹"
                            print(f"    ğŸ” [DEBUG] å…§æ–‡é è¦½: {preview_text}...")
                            
                            if "Access Denied" in body_text or "403 Forbidden" in body_text:
                                print(f"    ğŸ›‘ [CRITICAL] åµæ¸¬åˆ°å­˜å–è¢«æ‹’ï¼IP å¯èƒ½è¢«å°é–æˆ– Headless ç‰¹å¾µè¢«æŠ“ã€‚")
                            # ====================================================

                            driver.back()
                            time.sleep(1)
                            continue

                        # æª¢æŸ¥æ—¥æœŸæ˜¯å¦æ¯”æœ€æ–°æ—¥æœŸæ–°
                        current_date = datetime.strptime(date, '%Y/%m/%d')
                        if current_date <= latest_date:
                            print(f"  [{idx:2d}] {date} å·²å­˜åœ¨ï¼Œè·³é")
                            driver.back()
                            time.sleep(1)
                            continue

                        # æå–å…±æ©Ÿå…±è‰¦æ•¸é‡
                        aircraft, vessel = extract_numbers_from_text(body_text)

                        all_data.append([date, aircraft, vessel])
                        # æˆåŠŸè¼¸å‡º
                        print(f"  [{idx:2d}] {date} | å…±æ©Ÿ {aircraft:2d} | å…±è‰¦ {vessel:2d}")

                        # è¿”å›åˆ—è¡¨é 
                        driver.back()
                        time.sleep(1)

                    except Exception as e:
                        print(f"\n  [{idx:2d}] è™•ç†ç™¼ç”ŸéŒ¯èª¤: {e}")
                        try:
                            driver.get(page_url) # å˜—è©¦å›åˆ°åˆ—è¡¨é 
                            time.sleep(2)
                        except:
                            pass
                        continue

            except Exception as e:
                print(f"è™•ç†ç¬¬ {page} é å¤±æ•—: {e}")
                continue

    finally:
        driver.quit()
        print("\nç€è¦½å™¨å·²é—œé–‰")

    # å„²å­˜è³‡æ–™
    print(f"\n{'='*60}")
    if all_data:
        all_data.sort(key=lambda x: datetime.strptime(x[0], '%Y/%m/%d'))

        save_to_csv(all_data)

        print(f"\nå®Œæˆï¼å…±çˆ¬å– {len(all_data)} ç­†æ–°è³‡æ–™")
        print(f"\næœ€æ–° 5 ç­†è³‡æ–™:")
        print(f"{'æ—¥æœŸ':<12} | {'å…±æ©Ÿ':<4} | {'å…±è‰¦':<4}")
        print("-" * 30)
        for row in all_data[-5:]:
            print(f"{row[0]:<12} | {row[1]:>4} | {row[2]:>4}")
    else:
        print("æ²’æœ‰æ–°è³‡æ–™éœ€è¦å¯«å…¥")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
