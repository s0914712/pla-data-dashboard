import re
import requests
from datetime import datetime, timedelta
import PyPDF2
import io
import httpx
import json
import os
import pandas as pd



# CSV æ–‡ä»¶è·¯å¾‘
CSV_FILE = 'data/JapanandBattleship.csv'

# æ­·å²è¨˜éŒ„æ–‡ä»¶ï¼ˆé¿å…é‡è¤‡çˆ¬å–ï¼‰
HISTORY_FILE = 'data/japan_scrape_history.json'

APERTIS_API_KEY = os.getenv('APERTIS_API_KEY') or os.getenv('STIMA_API_KEY')
APERTIS_MODEL = 'gemini-2.5-flash-lite-preview-06-17'
APERTIS_BASE_URL = 'https://api.apertis.ai/v1'

# PDF åŸºç¤ URL
PDF_BASE_URL = 'https://www.mod.go.jp/js/pdf'

# è¦çˆ¬å–çš„å¤©æ•¸ï¼ˆå¾ä»Šå¤©å¾€å›æ¨ï¼‰
DAYS_TO_CHECK = 30

# æ¯å¤©æœ€å¤šæª¢æŸ¥å¹¾å€‹ PDF ç·¨è™Ÿ
MAX_PDF_NUM_PER_DAY = 10

# ç›®æ¨™å¹´ä»½
TARGET_YEAR = '2026'

# HTTP è«‹æ±‚è¨­å®š
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
}

# 0/1 åˆ†ææ¬„ä½
BINARY_FIELDS = ['ç©ºä¸­', 'è¯åˆæ¼”è¨“', 'è‰¦é€šé', 'èˆªæ¯æ´»å‹•', 'èˆ‡é‚£åœ‹', 'å®®å¤', 'å¤§ç¦¹', 'å°é¦¬', 'é€²', 'å‡º']

# =================================================

def generate_pdf_urls(start_date, end_date):
    """ç”Ÿæˆæ—¥æœŸç¯„åœå…§æ‰€æœ‰å¯èƒ½çš„ PDF URL"""
    urls = []
    current_date = start_date

    while current_date <= end_date:
        year = current_date.strftime('%Y')
        date_str = current_date.strftime('%Y%m%d')

        # æ¯å¤©å¯èƒ½æœ‰å¤šå€‹ PDF (01, 02, 03...)
        for num in range(1, MAX_PDF_NUM_PER_DAY + 1):
            pdf_filename = f"p{date_str}_{num:02d}.pdf"
            pdf_url = f"{PDF_BASE_URL}/{year}/{pdf_filename}"
            csv_date = current_date.strftime('%Y/%m/%d')
            urls.append({
                'url': pdf_url,
                'date': csv_date,
                'filename': pdf_filename
            })

        current_date += timedelta(days=1)

    return urls


def load_history():
    """è¼‰å…¥å·²è™•ç†çš„ PDF æ­·å²è¨˜éŒ„"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ è®€å–æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
    return {"processed_pdfs": []}


def save_history(history):
    """å„²å­˜å·²è™•ç†çš„ PDF æ­·å²è¨˜éŒ„"""
    try:
        os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ å„²å­˜æ­·å²è¨˜éŒ„å¤±æ•—: {e}")


def download_pdf(url):
    """ä¸‹è¼‰ PDF ä¸¦è¿”å›å…§å®¹ï¼Œ404 è¿”å› None"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            return response.content
        return None
    except Exception as e:
        print(f"    âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        return None


def is_target_navy_pdf(pdf_text):
    """åˆ¤æ–· PDF æ˜¯å¦ç‚ºä¸­åœ‹æˆ–ä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡å‹•å‘ç›¸é—œï¼Œæ’é™¤çµ±è¨ˆ/éå‹•å‘å ±å‘Š"""
    # æ’é™¤ï¼šæµ·è³Šå¯¾å‡¦å“¨æˆ’æ©Ÿæ´»å‹•å ±å‘Š
    if 'æµ·è³Šå¯¾å‡¦' in pdf_text and ('å“¨æˆ’æ©Ÿ' in pdf_text or 'ï¼°ï¼ï¼“ï¼£' in pdf_text or 'P-3C' in pdf_text):
        return False
    # æ’é™¤ï¼šç·Šæ€¥ç™ºé€²ï¼ˆã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«ï¼‰æ¶æ¬¡çµ±è¨ˆå ±å‘Š
    if 'ç·Šæ€¥ç™ºé€²' in pdf_text and any(kw in pdf_text for kw in ['å›æ•°', 'å®Ÿæ–½çŠ¶æ³', 'çŠ¶æ³ã«ã¤ã„ã¦', 'çµ±è¨ˆ']):
        return False
    china_keywords = ['ä¸­å›½', 'è‰¦è‰‡', 'æµ·è»', 'è­·è¡›è‰¦', 'é§†é€è‰¦', 'ç©ºæ¯', 'è£œçµ¦è‰¦']
    russia_keywords = ['ãƒ­ã‚·ã‚¢', 'ãƒ­ã‚·ã‚¢æµ·è»', 'ãƒ­ã‚·ã‚¢é€£é‚¦', 'éœ²æµ·è»', 'ã‚¦ãƒ€ãƒ­ã‚¤', 'ã‚¹ãƒ©ãƒ´ã‚¡', 'ã‚¹ãƒ†ãƒ¬ã‚°ã‚·ãƒãƒ¼']
    return any(kw in pdf_text for kw in china_keywords + russia_keywords)


def extract_text_from_pdf(pdf_url):
    """å¾ PDF URL æå–æ–‡æœ¬"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(pdf_url, timeout=30, headers=headers)
        response.raise_for_status()

        pdf_file = io.BytesIO(response.content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)

        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"

        return text.strip()
    except Exception as e:
        print(f"    âŒ PDF è®€å–å¤±æ•—: {e}")
        return None


def analyze_with_apertis(pdf_text, date):
    """ä½¿ç”¨ Apertis API åˆ†æ PDF æ–‡æœ¬"""

    prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆé–€åˆ†æä¸­åœ‹åŠä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡å‹•å‘çš„å°ˆå®¶ã€‚è«‹ä»”ç´°é–±è®€ä»¥ä¸‹æ—¥æœ¬é˜²è¡›çœç™¼å¸ƒçš„å ±å‘Šï¼Œåˆ¤æ–·æ˜¯å¦ç‚ºã€Œä¸­åœ‹æµ·è»æˆ–ä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡å‹•å‘ã€å ±å‘Šä¸¦æå–é—œéµè³‡è¨Šã€‚

**é‡è¦å‰æï¼šæœ¬åŠŸèƒ½è™•ç†ã€Œä¸­åœ‹æµ·è»æˆ–ä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡é€šé/æ´»å‹•ã€çš„å‹•å‘å ±å‘Šã€‚**
ä»¥ä¸‹é¡å‹çš„å ±å‘Šå±¬æ–¼ã€Œéè‰¦è‰‡å‹•å‘å ±å‘Šã€ï¼Œè«‹è¨­å®š valid_report=0ï¼Œæ‰€æœ‰æ•¸å€¼æ¬„ä½å¡« 0ï¼Œremark ç•™ç©ºï¼š
- æµ·è³Šå¯¾å‡¦ï¼ˆåæµ·ç›œï¼‰ä»»å‹™çš„å“¨æˆ’æ©Ÿæ´»å‹•å ±å‘Šï¼ˆP-3C ç­‰ï¼‰
- èˆªç©ºè‡ªè¡›éšŠç·Šæ€¥ç™ºé€²ï¼ˆã‚¹ã‚¯ãƒ©ãƒ³ãƒ–ãƒ«ï¼‰æ¶æ¬¡çµ±è¨ˆå ±å‘Š
- æ—¥æœ¬-ç¾åœ‹æˆ–å…¶ä»–åœ‹å®¶ä¹‹é–“çš„è¯åˆæ¼”ç¿’å…¬å‘Šï¼ˆéä¸­åœ‹/ä¿„ç¾…æ–¯åƒèˆ‡ï¼‰
- å…¶ä»–èˆ‡ä¸­åœ‹/ä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡å‹•å‘ç„¡é—œçš„å ±å‘Š

å ±å‘Šæ—¥æœŸï¼š{date}

å ±å‘Šå…§å®¹ï¼š
{pdf_text}

è‹¥ç¢ºèªç‚ºã€Œä¸­åœ‹æµ·è»æˆ–ä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡å‹•å‘ã€å ±å‘Šï¼Œè«‹æ ¹æ“šå ±å‘Šå…§å®¹åˆ¤æ–·ä»¥ä¸‹æ¬„ä½ï¼ˆé‡å°**ä¸­åœ‹æµ·è»åŠ/æˆ–ä¿„ç¾…æ–¯æµ·è»**ï¼‰ï¼š

**0/1 æ¬„ä½ï¼ˆæ˜¯/å¦ï¼‰ï¼š**
1. **ç©ºä¸­**ï¼šæ˜¯å¦æœ‰ç©ºä¸­æ´»å‹•ï¼ˆè‰¦è¼‰æ©Ÿã€ç›´å‡æ©Ÿç­‰ï¼‰ - å¡« 0 æˆ– 1
2. **è¯åˆæ¼”è¨“**ï¼šæ˜¯å¦èˆ‡å…¶ä»–åœ‹å®¶é€²è¡Œè¯åˆæ¼”ç¿’æˆ–è¨“ç·´ï¼ˆå«ä¸­ä¿„è¯åˆï¼‰ - å¡« 0 æˆ– 1
3. **è‰¦é€šé**ï¼šè‰¦è‰‡æ˜¯å¦é€šéç‰¹å®šæµ·åŸŸ - å¡« 0 æˆ– 1
4. **èˆªæ¯æ´»å‹•**ï¼šèˆªç©ºæ¯è‰¦æ˜¯å¦æœ‰ç›¸é—œæ´»å‹• - å¡« 0 æˆ– 1
5. **èˆ‡é‚£åœ‹**ï¼šè‰¦è‰‡æ˜¯å¦ç¶“éèˆ‡é‚£åœ‹å³¶é™„è¿‘ - å¡« 0 æˆ– 1
6. **å®®å¤**ï¼šè‰¦è‰‡æ˜¯å¦ç¶“éå®®å¤æµ·å³½ - å¡« 0 æˆ– 1
7. **å¤§ç¦¹**ï¼šè‰¦è‰‡æ˜¯å¦ç¶“éå¤§éš…æµ·å³½ - å¡« 0 æˆ– 1
8. **å°é¦¬**ï¼šè‰¦è‰‡æ˜¯å¦ç¶“éå°é¦¬æµ·å³½ - å¡« 0 æˆ– 1
9. **é€²**ï¼šè‰¦è‰‡æ˜¯å¦å‘æ±æµ·æ–¹å‘èˆªè¡Œï¼ˆå¾å¤ªå¹³æ´‹é€²å…¥æ±æµ·ï¼‰ - å¡« 0 æˆ– 1
10. **å‡º**ï¼šè‰¦è‰‡æ˜¯å¦å‘å¤ªå¹³æ´‹æ–¹å‘èˆªè¡Œï¼ˆå¾æ±æµ·å‡ºå‘å¤ªå¹³æ´‹ï¼‰ - å¡« 0 æˆ– 1

**æ–‡å­—æ¬„ä½ï¼š**
11. **åœ‹å®¶**ï¼šå¡«å¯«ã€Œä¸­åœ‹ã€ã€ã€Œä¿„ç¾…æ–¯ã€æˆ–ã€Œä¸­åœ‹ã€ä¿„ç¾…æ–¯ã€ï¼ˆè‹¥å…©åœ‹è‰¦è‰‡åŒæ™‚å‡ºç¾ï¼‰

12. **è‰¦å‹**ï¼šæå–è‰¦è‰‡çš„å…·é«”å‹è™Ÿï¼Œä½¿ç”¨ä¸­æ–‡åç¨±
    - ä¸­åœ‹è‰¦è‰‡ä¾‹å¦‚ï¼šæ—…æ´‹IIç´šé©…é€è‰¦ã€æ±Ÿé–‹ç´šè­·è¡›è‰¦ã€ç¾ä»£ç´šé©…é€è‰¦ã€ç¦æ± ç´šç¶œåˆè£œçµ¦è‰¦ç­‰
    - ä¿„ç¾…æ–¯è‰¦è‰‡ä¾‹å¦‚ï¼šçƒé”æ´›ä¼Šç´šé©…é€è‰¦ã€æ–¯æ‹‰ç“¦ç´šå·¡æ´‹è‰¦ã€å…‰æ¦®ç´šå·¡æ´‹è‰¦ã€ç„¡ç•ç´šé©…é€è‰¦ç­‰
    - å¦‚æœå ±å‘Šä¸­æåˆ°å¤šè‰˜è‰¦è‰‡ï¼Œè«‹åˆ—å‡ºæ‰€æœ‰å‹è™Ÿï¼Œç”¨é “è™Ÿã€Œã€ã€åˆ†éš”
    - å¦‚æœæ²’æœ‰æåˆ°å…·é«”å‹è™Ÿï¼Œå¡«ã€ŒæœªæåŠã€

13. **remark**ï¼šç”¨ç¹é«”ä¸­æ–‡æ’°å¯« 70 å­—ä»¥å…§çš„ç°¡è¦æè¿°ï¼Œæ¦‚è¿°æ­¤æ¬¡è‰¦è‰‡æ´»å‹•çš„é‡é»
    - åŒ…å«ï¼šåœ‹å®¶ã€è‰¦è‰‡æ•¸é‡ã€ç¶“éæµ·åŸŸã€èˆªè¡Œæ–¹å‘ã€ä¸»è¦æ´»å‹•
    - ä½¿ç”¨ç°¡æ½”çš„æ›¸é¢èª
    - ä¸è¶…é 70 å€‹ä¸­æ–‡å­—

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼Œåªå›è¦† JSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
{{
  "valid_report": 1,
  "åœ‹å®¶": "ä¸­åœ‹",
  "ç©ºä¸­": 0,
  "è¯åˆæ¼”è¨“": 0,
  "è‰¦é€šé": 0,
  "èˆªæ¯æ´»å‹•": 0,
  "èˆ‡é‚£åœ‹": 0,
  "å®®å¤": 0,
  "å¤§ç¦¹": 0,
  "å°é¦¬": 0,
  "é€²": 0,
  "å‡º": 0,
  "è‰¦å‹": "æ—…æ´‹IIç´šé©…é€è‰¦ã€æ±Ÿé–‹ç´šè­·è¡›è‰¦",
  "remark": "ä¸­åœ‹æµ·è»2è‰˜è‰¦è‰‡ç”±æ±æµ·ç¶“å°é¦¬æµ·å³½å‘æ—¥æœ¬æµ·èˆªè¡Œï¼ŒåŒ…æ‹¬æ—…æ´‹IIç´šé©…é€è‰¦åŠæ±Ÿé–‹ç´šè­·è¡›è‰¦ã€‚"
}}
"""

    try:
        with httpx.Client(timeout=60.0) as client:
            response = client.post(
                f"{APERTIS_BASE_URL}/chat/completions",
                headers={
                    "Authorization": APERTIS_API_KEY,
                    "Content-Type": "application/json"
                },
                json={
                    "model": APERTIS_MODEL,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": 1024,
                    "temperature": 0.1
                }
            )

            response.raise_for_status()
            result_json = response.json()

            response_text = result_json["choices"][0]["message"]["content"]

        # æå– JSON
        response_text = response_text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.startswith('```'):
            response_text = response_text[3:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        response_text = response_text.strip()

        result = json.loads(response_text)
        return result

    except httpx.HTTPStatusError as e:
        print(f"    âŒ Apertis API HTTP éŒ¯èª¤: {e.response.status_code} - {e.response.text}")
        return None
    except httpx.ConnectError as e:
        print(f"    âŒ Apertis API é€£æ¥éŒ¯èª¤: {e}")
        return None
    except Exception as e:
        print(f"    âŒ Apertis API åˆ†æå¤±æ•—: {e}")
        return None


def get_latest_date_from_csv():
    """å¾ CSV è®€å–æœ€æ–°æ—¥æœŸï¼ˆå«æ—¥æœ¬é˜²è¡›çœæ•¸æ“šçš„æ—¥æœŸï¼‰"""
    try:
        if not os.path.exists(CSV_FILE):
            print(f"âš ï¸ CSV æª”æ¡ˆä¸å­˜åœ¨: {CSV_FILE}")
            return None

        df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')

        if df.empty or 'date' not in df.columns:
            return None

        df_filtered = df[(df['è‰¦å‹'].notna() & (df['è‰¦å‹'] != '')) |
                         (df['remark'].notna() & (df['remark'] != ''))]

        if df_filtered.empty:
            return None

        dates = pd.to_datetime(df_filtered['date'], format='%Y/%m/%d', errors='coerce')
        latest_date = dates.max()

        if pd.isna(latest_date):
            return None

        return latest_date

    except Exception as e:
        print(f"è®€å– CSV æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


def check_date_data_validity(date, df):
    """æª¢æŸ¥æŒ‡å®šæ—¥æœŸçš„è³‡æ–™æ˜¯å¦æœ‰æ•ˆï¼ˆè‡³å°‘æœ‰ä¸€å€‹0/1æ¬„ä½ç‚º1ï¼‰"""
    try:
        mask = df['date'] == date

        if not mask.any():
            return False

        row = df[mask].iloc[0]

        for field in BINARY_FIELDS:
            if field in row and pd.notna(row[field]):
                value = str(row[field]).strip()
                if value in ['1', '1.0']:
                    return True

        return False

    except Exception as e:
        print(f"      âš ï¸  æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False


def update_csv(date, data):
    """æ›´æ–° CSV æ–‡ä»¶ä¸­æŒ‡å®šæ—¥æœŸçš„è³‡æ–™"""
    try:
        if not os.path.exists(CSV_FILE):
            print(f"âŒ CSV æª”æ¡ˆä¸å­˜åœ¨: {CSV_FILE}")
            return False

        df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')

        mask = df['date'] == date

        if not mask.any():
            print(f"      âš ï¸  æ‰¾ä¸åˆ°æ—¥æœŸ {date} çš„è¡Œ")
            return False

        for key, value in data.items():
            if key in df.columns:
                df.loc[mask, key] = value

        df.to_csv(CSV_FILE, index=False, encoding='utf-8-sig')
        print(f"      âœ“ å·²æ›´æ–°æ—¥æœŸ {date} çš„è³‡æ–™")
        return True

    except Exception as e:
        print(f"      âŒ æ›´æ–°è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»ç¨‹å¼"""
    import time

    print("="*60)
    print("æ—¥æœ¬é˜²è¡›çœä¸­åœ‹/ä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡å‹•å‘çˆ¬èŸ² V6 - ç›´æ¥ä¸‹è¼‰ PDF ç‰ˆ")
    print("="*60)

    if not APERTIS_API_KEY:
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­ç½® APERTIS_API_KEY ç’°å¢ƒè®Šé‡")
        return

    # è®€å– CSV
    print("\næ­£åœ¨è®€å– CSV...")
    try:
        df = pd.read_csv(CSV_FILE, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸè®€å–: {CSV_FILE}")
        print(f"ğŸ“Š ç¸½è¡Œæ•¸: {len(df)}")
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•—: {e}")
        return

    latest_date = get_latest_date_from_csv()
    if latest_date:
        print(f"ğŸ“… æœ€æ–°æ—¥æœ¬é˜²è¡›çœè³‡æ–™æ—¥æœŸ: {latest_date.strftime('%Y/%m/%d')}")
    else:
        print(f"ğŸ“… ç„¡ç¾æœ‰æ—¥æœ¬é˜²è¡›çœè³‡æ–™")
        latest_date = datetime.min

    # è¼‰å…¥æ­·å²è¨˜éŒ„
    history = load_history()
    processed_set = set(history.get("processed_pdfs", []))
    print(f"ğŸ“‚ å·²è™•ç†éçš„ PDF: {len(processed_set)} å€‹")

    print(f"\n{'='*60}")
    print("ğŸš€ é–‹å§‹çˆ¬å–æ—¥æœ¬é˜²è¡›çœè³‡æ–™ï¼ˆç›´æ¥ä¸‹è¼‰ PDFï¼‰...")
    print(f"{'='*60}\n")

    # ç”Ÿæˆè¦æª¢æŸ¥çš„ PDF URL åˆ—è¡¨
    end_date = datetime.now()
    start_date = end_date - timedelta(days=DAYS_TO_CHECK)

    # ç¢ºä¿åœ¨ç›®æ¨™å¹´ä»½ç¯„åœå…§
    if TARGET_YEAR:
        year_start = datetime(int(TARGET_YEAR), 1, 1)
        if start_date < year_start:
            start_date = year_start

    print(f"ğŸ“… æª¢æŸ¥æ—¥æœŸç¯„åœ: {start_date.strftime('%Y/%m/%d')} ~ {end_date.strftime('%Y/%m/%d')}")

    pdf_urls = generate_pdf_urls(start_date, end_date)
    print(f"ğŸ“‹ å…±ç”Ÿæˆ {len(pdf_urls)} å€‹å¯èƒ½çš„ PDF URL\n")

    updated_pdfs = 0
    found_pdfs = 0
    skipped_history = 0

    for idx, pdf_info in enumerate(pdf_urls, 1):
        pdf_url = pdf_info['url']
        date = pdf_info['date']
        filename = pdf_info['filename']

        # æª¢æŸ¥æ­·å²è¨˜éŒ„ï¼Œè·³éå·²è™•ç†çš„ PDF
        if filename in processed_set:
            skipped_history += 1
            continue

        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æœ‰æ•ˆè³‡æ–™
        try:
            current_date = datetime.strptime(date, '%Y/%m/%d')
            if current_date <= latest_date:
                is_valid = check_date_data_validity(date, df)
                if is_valid:
                    continue  # éœé»˜è·³éå·²æœ‰è³‡æ–™çš„æ—¥æœŸ
        except:
            pass

        # å˜—è©¦ä¸‹è¼‰ PDF
        pdf_content = download_pdf(pdf_url)

        if not pdf_content:
            continue  # 404 æˆ–ä¸‹è¼‰å¤±æ•—ï¼Œéœé»˜è·³é

        found_pdfs += 1
        # è¨˜éŒ„åˆ°æ­·å²ï¼ˆä¸è«–å¾ŒçºŒåˆ†æçµæœå¦‚ä½•ï¼Œéƒ½ä¸å†é‡è¤‡ä¸‹è¼‰ï¼‰
        processed_set.add(filename)

        print(f"[{found_pdfs}] ğŸ“¥ æ‰¾åˆ°: {filename}")
        print(f"    ğŸ“… æ—¥æœŸ: {date}")

        # è§£æ PDF
        try:
            pdf_file = io.BytesIO(pdf_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)

            pdf_text = ""
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:
                    pdf_text += page_text + "\n"

            pdf_text = pdf_text.strip()
        except Exception as e:
            print(f"    âŒ PDF è§£æå¤±æ•—: {e}\n")
            continue

        if not pdf_text:
            print(f"    âš ï¸ PDF ç„¡æ–‡å­—å…§å®¹\n")
            continue

        # æª¢æŸ¥æ˜¯å¦ç‚ºä¸­åœ‹/ä¿„ç¾…æ–¯æµ·è»ç›¸é—œ
        if not is_target_navy_pdf(pdf_text):
            print(f"    â­ï¸ éä¸­åœ‹/ä¿„ç¾…æ–¯æµ·è»è‰¦è‰‡ç›¸é—œ\n")
            continue

        print(f"    ğŸ“„ æå–æ–‡æœ¬: {len(pdf_text)} å­—")

        # AI åˆ†æ
        print(f"    ğŸ¤– AI åˆ†æä¸­...", end=" ")
        analysis = analyze_with_apertis(pdf_text, date)

        if not analysis:
            print("å¤±æ•—\n")
            continue

        print("âœ“")

        # æª¢æŸ¥ AI æ˜¯å¦åˆ¤å®šç‚ºæœ‰æ•ˆè‰¦è‰‡å‹•å‘å ±å‘Š
        if analysis.get('valid_report', 1) == 0:
            print(f"    â­ï¸ AI åˆ¤å®šç‚ºéè‰¦è‰‡å‹•å‘å ±å‘Šï¼Œè·³é\n")
            continue

        # ç§»é™¤ valid_report æ¬„ä½ï¼Œä¸å¯«å…¥ CSV
        analysis.pop('valid_report', None)

        # æ›´æ–° CSV
        if update_csv(date, analysis):
            updated_pdfs += 1

        country = analysis.get('åœ‹å®¶', 'ä¸­åœ‹')
        print(f"    âœ… {date} ({country}):")
        binary_str = " | ".join([f"{k}:{v}" for k, v in analysis.items() if k in BINARY_FIELDS and v == 1])
        if binary_str:
            print(f"       {binary_str}")
        if 'è‰¦å‹' in analysis and analysis['è‰¦å‹'] and analysis['è‰¦å‹'] != 'æœªæåŠ':
            print(f"       è‰¦å‹: {analysis['è‰¦å‹']}")
        if 'remark' in analysis and analysis['remark']:
            remark_display = analysis['remark'][:50] + '...' if len(analysis['remark']) > 50 else analysis['remark']
            print(f"       å‚™è¨»: {remark_display}")
        print()

        time.sleep(1.5)  # é¿å…è«‹æ±‚éå¿«

    # å„²å­˜æ­·å²è¨˜éŒ„
    history["processed_pdfs"] = sorted(processed_set)
    save_history(history)

    print(f"\n{'='*60}")
    print(f"ğŸ“Š æƒæå®Œæˆ:")
    print(f"   æ­·å²è·³é: {skipped_history} å€‹ï¼ˆå·²è™•ç†éï¼‰")
    print(f"   æ–°æ‰¾åˆ° PDF: {found_pdfs} å€‹")
    print(f"   æ›´æ–°è³‡æ–™: {updated_pdfs} ç­†")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
