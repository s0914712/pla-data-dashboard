#!/usr/bin/env python3
"""
===============================================================================
ä¸»æ›´æ–°è…³æœ¬ / Main Update Script
===============================================================================
æ•´åˆ CNA / Xinhua çˆ¬èŸ²ã€Grok åˆ†é¡å™¨èˆ‡ GitHub æ›´æ–°å™¨
"""
import argparse
import json
import logging
import sys
import io
import traceback
from pathlib import Path
from datetime import datetime
import os
# ---------------------------------------------------------------------------
# Path è¨­å®š
# ---------------------------------------------------------------------------
# æ·»åŠ çˆ¶ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))
from scrapers.cna_scraper import CNAScraper
from scrapers.xinhua_scraper import XinhuaTWScraper
from scrapers.weibo_scraper import WeiboScraper
from classifiers.grok_classifier import GrokNewsClassifier
from updaters.github_updater import GitHubUpdater
from updaters.naval_transit_updater import NavalTransitUpdater
# ---------------------------------------------------------------------------
# Helper: JSON åˆä½µ
# ---------------------------------------------------------------------------
def _load_existing_json(path):
    """è¼‰å…¥æ—¢æœ‰ JSON æª”æ¡ˆï¼Œè‹¥ä¸å­˜åœ¨æˆ–æ ¼å¼éŒ¯èª¤å‰‡å›å‚³ç©ºåˆ—è¡¨"""
    try:
        if path.exists():
            with path.open("r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
    except (json.JSONDecodeError, Exception):
        pass
    return []


def _merge_articles(existing, new_articles):
    """åˆä½µæ–‡ç« åˆ—è¡¨ï¼Œä»¥ original_article.url å»é‡"""
    def _get_url(article):
        url = article.get("original_article", {}).get("url", "")
        if not url:
            url = article.get("url", "")
        return url

    seen_urls = set()
    merged = []
    # æ–°æ–‡ç« å„ªå…ˆï¼ˆåˆ†é¡å¯èƒ½æ›´æ–°ï¼‰ï¼Œå†è£œä¸ŠèˆŠæ–‡ç« 
    for article in new_articles:
        url = _get_url(article)
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged.append(article)
        elif not url:
            merged.append(article)
    for article in existing:
        url = _get_url(article)
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged.append(article)
    return merged


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description="Daily News Update Script")
    parser.add_argument("--days", type=int, default=7, help="Days back to scrape")
    parser.add_argument("--no-push", action="store_true", help="Skip GitHub push")
    args = parser.parse_args()
    # -------------------------------------------------------------------
    # è¨­å®šæ—¥èªŒæ•ç² (åŒæ™‚è¼¸å‡ºåˆ° console å’Œè¨˜æ†¶é«”)
    # -------------------------------------------------------------------
    start_time = datetime.now()
    log_capture = io.StringIO()
    log_handler = logging.StreamHandler(log_capture)
    log_handler.setLevel(logging.INFO)
    log_handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))
    logger = logging.getLogger('daily_update')
    logger.setLevel(logging.INFO)
    logger.addHandler(log_handler)
    print("=" * 70)
    print(f"ğŸš€ Starting Daily News Update - {start_time:%Y-%m-%d %H:%M:%S}")
    print(f"ğŸ“… Scraping news from past {args.days} days")
    print("=" * 70)
    logger.info(f"Starting Daily News Update - {start_time:%Y-%m-%d %H:%M:%S}")
    logger.info(f"Days back: {args.days}")
    all_articles = []
    stats = {
        "timestamp": start_time.isoformat(),
        "days_back": args.days,
        "sources": {}
    }
    # -----------------------------------------------------------------------
    # 1. CNA
    # -----------------------------------------------------------------------
    print("\n[1/6] çˆ¬å–ä¸­å¤®ç¤¾æ–°è...")
    try:
        with CNAScraper(delay=1.0) as cna:
            cna_articles = cna.run(days_back=args.days)
            all_articles.extend(cna_articles)
            stats["sources"]["cna"] = {
                "scraped": len(cna_articles),
                "status": "success"
            }
            print(f"âœ“ CNA: {len(cna_articles)} ç¯‡æ–°è")
            logger.info(f"CNA: scraped {len(cna_articles)} articles")
    except Exception as e:
        print(f"âœ— CNA Error: {e}")
        logger.error(f"CNA Error: {e}")
        stats["sources"]["cna"] = {
            "status": "failed",
            "error": str(e)
        }
    # -----------------------------------------------------------------------
    # 2. Xinhuaï¼ˆå¯é¸ï¼‰
    # -----------------------------------------------------------------------
    print("\n[2/6] çˆ¬å–æ–°è¯ç¤¾æ–°è...")
    try:
        with XinhuaTWScraper() as xinhua:
            xinhua_articles = xinhua.run(days_back=args.days)
            all_articles.extend(xinhua_articles)

            stats["sources"]["xinhua"] = {
                "scraped": len(xinhua_articles),
                "status": "success"
            }

            print(f"âœ“ Xinhua: {len(xinhua_articles)} ç¯‡æ–°è")
            logger.info(f"Xinhua: scraped {len(xinhua_articles)} articles")
    except Exception as e:
        print(f"âœ— Xinhua Error: {e}")
        logger.error(f"Xinhua Error: {e}")
        stats["sources"]["xinhua"] = {
            "status": "failed",
            "error": str(e)
        }
    # -----------------------------------------------------------------------
    # 3. Weiboï¼ˆæ±éƒ¨æˆ°å€ï¼‰
    # -----------------------------------------------------------------------
    print("\n[3/6] çˆ¬å–å¾®åšè²¼æ–‡...")
    try:
        with WeiboScraper() as weibo:
            weibo_articles = weibo.run(days_back=args.days)
            all_articles.extend(weibo_articles)

            stats["sources"]["weibo"] = {
                "scraped": len(weibo_articles),
                "status": "success"
            }

            print(f"âœ“ Weibo: {len(weibo_articles)} ç¯‡è²¼æ–‡")
            logger.info(f"Weibo: scraped {len(weibo_articles)} posts")
    except Exception as e:
        print(f"âœ— Weibo Error: {e}")
        logger.error(f"Weibo Error: {e}")
        stats["sources"]["weibo"] = {
            "status": "failed",
            "error": str(e)
        }
    if not all_articles:
        print("\nâŒ No articles scraped. Exiting.")
        logger.error("No articles scraped. Exiting.")
        # å³ä½¿å¤±æ•—ä¹Ÿå¯«å…¥ log
        _save_execution_log(stats, start_time, log_capture, success=False)
        sys.exit(1)
    print(f"\nğŸ“Š Total articles scraped: {len(all_articles)}")
    logger.info(f"Total articles scraped: {len(all_articles)}")
    # -----------------------------------------------------------------------
    # 4. Grok å»é‡ + åˆ†é¡
    # -----------------------------------------------------------------------
    print("\n[4/6] ä½¿ç”¨ Grok é€²è¡Œå»é‡èˆ‡æ–°èåˆ†é¡...")
    api_key = os.environ.get("GROK_API_KEY")
    if not api_key:
        print("âŒ GROK_API_KEY not found in environment")
        sys.exit(1)
    try:
        with GrokNewsClassifier(api_key) as classifier:
            # å»é‡ï¼šå…ˆç”¨ LLM è­˜åˆ¥é‡è¤‡/é«˜åº¦ç›¸ä¼¼æ–‡ç« 
            deduped = classifier.deduplicate_batch(all_articles)
            stats["deduplication"] = {
                "before": len(all_articles),
                "after": len(deduped),
                "removed": len(all_articles) - len(deduped),
            }
            logger.info(
                f"Dedup: {len(all_articles)} â†’ {len(deduped)} "
                f"(removed {len(all_articles) - len(deduped)})"
            )
            # åˆ†é¡
            classified = classifier.classify_batch(deduped, delay=1.0)
            relevant = classifier.filter_relevant(classified)
            stats["classification"] = {
                "total": len(classified),
                "relevant": len(relevant),
                "status": "success"
            }
            print(f"âœ“ Classified: {len(classified)} ç¯‡")
            print(f"âœ“ Relevant: {len(relevant)} ç¯‡")
            logger.info(f"Classified: {len(classified)}, Relevant: {len(relevant)}")
    except Exception as e:
        print(f"âœ— Classification Error: {e}")
        logger.error(f"Classification Error: {e}")
        stats["classification"] = {
            "status": "failed",
            "error": str(e)
        }
        _save_execution_log(stats, start_time, log_capture, success=False)
        sys.exit(1)
    # -----------------------------------------------------------------------
    # 5. å„²å­˜çµæœï¼ˆåˆä½µæ—¢æœ‰è³‡æ–™ï¼Œé¿å…è¦†è“‹å…¶ä»–çˆ¬èŸ²çš„æˆæœï¼‰
    # -----------------------------------------------------------------------
    print("\n[5/6] ä¿å­˜æ•¸æ“š...")
    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)
    classified_file = data_dir / "news_classified.json"
    relevant_file = data_dir / "news_relevant.json"
    # è®€å–æ—¢æœ‰è³‡æ–™ä¸¦åˆä½µï¼ˆä»¥ url å»é‡ï¼‰
    existing_classified = _load_existing_json(classified_file)
    existing_relevant = _load_existing_json(relevant_file)
    merged_classified = _merge_articles(existing_classified, classified)
    merged_relevant = _merge_articles(existing_relevant, relevant)
    with classified_file.open("w", encoding="utf-8") as f:
        json.dump(merged_classified, f, ensure_ascii=False, indent=2)
    with relevant_file.open("w", encoding="utf-8") as f:
        json.dump(merged_relevant, f, ensure_ascii=False, indent=2)
    print(f"âœ“ Saved: {classified_file} ({len(merged_classified)} articles)")
    print(f"âœ“ Saved: {relevant_file} ({len(merged_relevant)} articles)")
    logger.info(f"Saved: {classified_file}, {relevant_file}")
    # -----------------------------------------------------------------------
    # 5b. æ›´æ–° naval_transits.csvï¼ˆForeign_battleship â†’ è»è‰¦é€šéè¨˜éŒ„ï¼‰
    # -----------------------------------------------------------------------
    print("\n[5b/6] æ›´æ–°è»è‰¦é€šéè¨˜éŒ„...")
    try:
        naval_csv = data_dir / "naval_transits.csv"
        sortie_csv = data_dir / "merged_comprehensive_data_M.csv"
        transit_updater = NavalTransitUpdater(
            csv_path=str(naval_csv),
            sortie_csv_path=str(sortie_csv) if sortie_csv.exists() else None,
        )
        transit_count = transit_updater.update_from_classified(merged_classified)
        stats["naval_transits"] = {
            "new_entries": transit_count,
            "status": "success",
        }
        print(f"âœ“ Naval transits: {transit_count} new entries")
        logger.info(f"Naval transits updated: {transit_count} new entries")
        # -----------------------------------------------------------------
        # 5c. å°‡ naval_transits.csv å…¨éƒ¨è¨˜éŒ„è½‰ç‚º JSONï¼Œåˆä½µåˆ°åˆ†é¡çµæœä¸­
        # -----------------------------------------------------------------
        print("\n[5c/6] åŒæ­¥è»è‰¦é€šéè¨˜éŒ„åˆ° JSON...")
        transit_articles = transit_updater.csv_to_json_articles()
        if transit_articles:
            merged_classified = _merge_articles(merged_classified, transit_articles)
            merged_relevant = _merge_articles(merged_relevant, transit_articles)
            with classified_file.open("w", encoding="utf-8") as f:
                json.dump(merged_classified, f, ensure_ascii=False, indent=2)
            with relevant_file.open("w", encoding="utf-8") as f:
                json.dump(merged_relevant, f, ensure_ascii=False, indent=2)
            print(f"âœ“ Synced {len(transit_articles)} transit records to JSON")
            logger.info(f"Synced {len(transit_articles)} transit records to JSON")
    except Exception as e:
        print(f"âœ— Naval transit update error: {e}")
        logger.error(f"Naval transit update error: {e}")
        stats["naval_transits"] = {"status": "failed", "error": str(e)}
    # å„²å­˜åŸ·è¡Œæ—¥èªŒåˆ° data/logs/
    log_file = _save_execution_log(stats, start_time, log_capture, success=True)
    # -----------------------------------------------------------------------
    # 6. GitHub æ¨é€
    # -----------------------------------------------------------------------
    if args.no_push:
        print("\n[6/6] Skipping GitHub push (--no-push)")
        return
    print("\n[6/6] æ¨é€åˆ° GitHub...")
    try:
        updater = GitHubUpdater()
        updater.configure_git(
            name="PLA Data Bot",
            email="bot@example.com"
        )
        updater.create_summary_log(stats, "data/last_update.json")
        # æ”¶é›†è¦æ¨é€çš„æª”æ¡ˆï¼ˆåŒ…å« logï¼‰
        push_files = [
            "data/news_classified.json",
            "data/news_relevant.json",
            "data/naval_transits.csv",
            "data/last_update.json",
        ]
        if log_file and Path(log_file).exists():
            push_files.append(str(log_file))
        success = updater.commit_and_push_data(
            data_files=push_files,
            message=f"ğŸ¤– Auto-update: {datetime.now():%Y-%m-%d %H:%M}"
        )
        print("âœ“ Pushed to GitHub" if success else "âš ï¸  No changes to push")
    except Exception as e:
        print(f"âœ— GitHub Error: {e}")
        stats["github_push"] = {
            "status": "failed",
            "error": str(e)
        }
    print("\n" + "=" * 70)
    print("âœ… Update completed successfully!")
    print("=" * 70)
# ---------------------------------------------------------------------------
# Helper: å„²å­˜åŸ·è¡Œæ—¥èªŒ
# ---------------------------------------------------------------------------
def _save_execution_log(stats, start_time, log_capture, success=True):
    """å°‡åŸ·è¡Œçµ±è¨ˆèˆ‡æ—¥èªŒè¼¸å‡ºå¯«å…¥ data/logs/<timestamp>.json"""
    try:
        end_time = datetime.now()
        logs_dir = Path("data/logs")
        logs_dir.mkdir(parents=True, exist_ok=True)
        log_data = {
            "run_timestamp": start_time.isoformat(),
            "end_timestamp": end_time.isoformat(),
            "duration_seconds": round((end_time - start_time).total_seconds(), 1),
            "success": success,
            "stats": stats,
            "console_output": log_capture.getvalue()
        }
        log_filename = logs_dir / f"{start_time:%Y%m%d_%H%M%S}.json"
        with log_filename.open("w", encoding="utf-8") as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ Log saved: {log_filename}")
        return str(log_filename)
    except Exception as e:
        print(f"âš ï¸  Failed to save log: {e}")
        return None
if __name__ == "__main__":
    main()
