#!/usr/bin/env python3
"""
===============================================================================
PLA Data Dashboard æ–°èçˆ¬èŸ²èˆ‡è‡ªå‹•æ›´æ–°ä¸»ç¨‹å¼
News Scraper & Auto-Update Main Script
===============================================================================

æ¯æ—¥è‡ªå‹•åŸ·è¡Œæµç¨‹ï¼š
1. çˆ¬å–æ–°è¯ç¤¾å’Œä¸­å¤®ç¤¾æ–°è
2. ä½¿ç”¨ Grok API åˆ†é¡å’Œæƒ…ç·’åˆ†æ
3. æ›´æ–° CSV æ•¸æ“šé›†
4. æ¨é€åˆ° GitHub

ä½¿ç”¨æ–¹æ³•ï¼š
    python main.py                    # å®Œæ•´åŸ·è¡Œ
    python main.py --scrape-only      # åªçˆ¬å–ä¸åˆ†é¡
    python main.py --no-push          # ä¸æ¨é€åˆ° GitHub
    python main.py --days 3           # çˆ¬å–éå» 3 å¤©

ç’°å¢ƒè®Šæ•¸ï¼š
    GROK_API_KEY    - Grok API å¯†é‘°
    GITHUB_TOKEN    - GitHub Tokenï¼ˆGitHub Actions è‡ªå‹•æä¾›ï¼‰
"""

import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path

# æ·»åŠ  scripts ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from config import Config
from scrapers import XinhuaTWScraper, CNAScraper
from classifiers import GrokNewsClassifier
from updaters import CSVUpdater, GitHubUpdater


def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(
        description='PLA Data Dashboard News Scraper & Updater'
    )
    parser.add_argument(
        '--days', type=int, default=7,
        help='çˆ¬å–éå»å¹¾å¤©çš„æ–°è (default: 7)'
    )
    parser.add_argument(
        '--scrape-only', action='store_true',
        help='åªçˆ¬å–æ–°èï¼Œä¸é€²è¡Œåˆ†é¡'
    )
    parser.add_argument(
        '--no-push', action='store_true',
        help='ä¸æ¨é€åˆ° GitHub'
    )
    parser.add_argument(
        '--output', type=str, default=None,
        help='è¼¸å‡º CSV è·¯å¾‘'
    )
    parser.add_argument(
        '--debug', action='store_true',
        help='å•Ÿç”¨èª¿è©¦æ¨¡å¼'
    )
    return parser.parse_args()


def main():
    """ä¸»åŸ·è¡Œæµç¨‹"""
    args = parse_args()
    
    # åˆå§‹åŒ–æ—¥èªŒ
    log = {
        'timestamp': datetime.now().isoformat(),
        'args': vars(args),
        'steps': [],
        'status': 'running'
    }
    
    print("=" * 70)
    print("PLA Data Dashboard News Scraper & Updater")
    print(f"Started at: {log['timestamp']}")
    print("=" * 70)
    
    # é©—è­‰é…ç½®
    errors = Config.validate()
    if errors and not args.scrape_only:
        print(f"\nâš ï¸  Configuration errors: {errors}")
        print("Set GROK_API_KEY environment variable or use --scrape-only")
        if not args.debug:
            sys.exit(1)
    
    all_news = []
    
    # ================================================================
    # Step 1: çˆ¬å–æ–°è
    # ================================================================
    print("\n" + "=" * 70)
    print("Step 1: Scraping news...")
    print("=" * 70)
    
    # æ–°è¯ç¤¾çˆ¬èŸ²
    try:
        print(f"\n[1.1] Scraping Xinhua Taiwan ({Config.XINHUA_URL})...")
        with XinhuaTWScraper() as scraper:
            xinhua_news = scraper.run(days_back=args.days)
        print(f"      Found {len(xinhua_news)} articles from Xinhua")
        all_news.extend(xinhua_news)
        log['steps'].append({'scraper': 'xinhua', 'count': len(xinhua_news), 'status': 'success'})
    except Exception as e:
        print(f"      âŒ Xinhua scraper error: {e}")
        log['steps'].append({'scraper': 'xinhua', 'error': str(e), 'status': 'failed'})
    
    # ä¸­å¤®ç¤¾çˆ¬èŸ²
    try:
        print(f"\n[1.2] Scraping CNA Military ({Config.CNA_SEARCH_URL})...")
        with CNAScraper() as scraper:
            cna_news = scraper.run(days_back=args.days)
        print(f"      Found {len(cna_news)} articles from CNA")
        all_news.extend(cna_news)
        log['steps'].append({'scraper': 'cna', 'count': len(cna_news), 'status': 'success'})
    except Exception as e:
        print(f"      âŒ CNA scraper error: {e}")
        log['steps'].append({'scraper': 'cna', 'error': str(e), 'status': 'failed'})
    
    print(f"\nğŸ“Š Total news scraped: {len(all_news)}")
    
    if not all_news:
        print("\nâš ï¸  No news found. Exiting.")
        log['status'] = 'no_news'
        return log
    
    # å¦‚æœåªçˆ¬å–ï¼Œä¿å­˜åŸå§‹æ•¸æ“šä¸¦é€€å‡º
    if args.scrape_only:
        raw_output = Path('data') / 'raw_news.json'
        raw_output.parent.mkdir(parents=True, exist_ok=True)
        with open(raw_output, 'w', encoding='utf-8') as f:
            json.dump({
                'scraped_at': datetime.now().isoformat(),
                'total': len(all_news),
                'articles': all_news
            }, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… Raw news saved to {raw_output}")
        log['status'] = 'scrape_only_complete'
        return log
    
    # ================================================================
    # Step 2: LLM åˆ†é¡å’Œæƒ…ç·’åˆ†æ
    # ================================================================
    print("\n" + "=" * 70)
    print("Step 2: Classifying with Grok API...")
    print("=" * 70)
    
    classified_news = []
    try:
        with GrokNewsClassifier(api_key=Config.GROK_API_KEY) as classifier:
            classified_news = classifier.classify_batch(all_news, delay=1.5)
            relevant_news = classifier.filter_relevant(classified_news)
        
        print(f"\nğŸ“Š Classification results:")
        print(f"   - Total processed: {len(classified_news)}")
        print(f"   - Relevant news: {len(relevant_news)}")
        
        # çµ±è¨ˆé¡åˆ¥åˆ†ä½ˆ
        categories = {}
        for item in classified_news:
            cat = item.get('category', 'Unknown')
            categories[cat] = categories.get(cat, 0) + 1
        print(f"   - Categories: {categories}")
        
        # çµ±è¨ˆæƒ…ç·’åˆ†ä½ˆ
        sentiments = {'positive': 0, 'neutral': 0, 'negative': 0}
        for item in classified_news:
            label = item.get('sentiment_label', 'neutral')
            sentiments[label] = sentiments.get(label, 0) + 1
        print(f"   - Sentiments: {sentiments}")
        
        log['steps'].append({
            'classifier': 'grok',
            'total': len(classified_news),
            'relevant': len(relevant_news),
            'categories': categories,
            'sentiments': sentiments,
            'status': 'success'
        })
        
    except Exception as e:
        print(f"\nâŒ Classification error: {e}")
        log['steps'].append({'classifier': 'grok', 'error': str(e), 'status': 'failed'})
        relevant_news = []
    
    if not relevant_news:
        print("\nâš ï¸  No relevant news found after classification.")
        log['status'] = 'no_relevant_news'
    
    # ================================================================
    # Step 3: æ›´æ–° CSV
    # ================================================================
    print("\n" + "=" * 70)
    print("Step 3: Updating CSV...")
    print("=" * 70)
    
    csv_path = args.output or str(Config.CSV_PATH)
    
    try:
        updater = CSVUpdater(csv_path)
        updated_count = updater.update_from_classified(classified_news)
        output_path = updater.save()
        
        stats = updater.get_stats()
        print(f"\nğŸ“Š CSV Update results:")
        print(f"   - Records updated: {updated_count}")
        print(f"   - Total rows: {stats['total_rows']}")
        print(f"   - Date range: {stats['date_range']}")
        
        log['steps'].append({
            'updater': 'csv',
            'updated': updated_count,
            'output': output_path,
            'stats': stats,
            'status': 'success'
        })
        
    except Exception as e:
        print(f"\nâŒ CSV update error: {e}")
        log['steps'].append({'updater': 'csv', 'error': str(e), 'status': 'failed'})
        output_path = None
    
    # ================================================================
    # Step 4: æ¨é€åˆ° GitHub
    # ================================================================
    if not args.no_push and output_path and Config.ENABLE_GITHUB_PUSH:
        print("\n" + "=" * 70)
        print("Step 4: Pushing to GitHub...")
        print("=" * 70)
        
        try:
            github = GitHubUpdater('.')
            github.configure_git()
            
            # ä¿å­˜æ—¥èªŒ
            log_file = github.create_run_log(log)
            
            # æäº¤æ–‡ä»¶
            files_to_commit = [
                output_path,
                log_file
            ]
            
            success = github.commit_and_push(
                files_to_commit,
                message=f"Auto-update: {datetime.now().strftime('%Y-%m-%d')} ({updated_count} records)"
            )
            
            if success:
                print("\nâœ… Successfully pushed to GitHub")
                log['steps'].append({'github': 'push', 'status': 'success'})
            else:
                print("\nâš ï¸  GitHub push failed")
                log['steps'].append({'github': 'push', 'status': 'failed'})
                
        except Exception as e:
            print(f"\nâŒ GitHub error: {e}")
            log['steps'].append({'github': 'push', 'error': str(e), 'status': 'failed'})
    else:
        print("\nâ­ï¸  Skipping GitHub push")
    
    # ================================================================
    # å®Œæˆ
    # ================================================================
    log['status'] = 'completed'
    log['completed_at'] = datetime.now().isoformat()
    
    print("\n" + "=" * 70)
    print("âœ… Update completed!")
    print(f"   Finished at: {log['completed_at']}")
    print("=" * 70)
    
    return log


if __name__ == "__main__":
    main()
