#!/usr/bin/env python3
"""
===============================================================================
ä¸»æ›´æ–°è…³æœ¬ / Main Update Script
===============================================================================

æ•´åˆ CNA/Xinhua çˆ¬èŸ²ã€Grok åˆ†é¡å™¨å’Œ GitHub æ›´æ–°å™¨
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from scrapers.cna_scraper import CNAScraper
# from scrapers.xinhua_scraper import XinhuaScraper  # å¦‚æœæœ‰çš„è©±
from classifiers.grok_classifier import GrokNewsClassifier
from updaters.github_updater import GitHubUpdater


def main():
    parser = argparse.ArgumentParser(description='Daily News Update Script')
    parser.add_argument('--days', type=int, default=7, help='Days back to scrape')
    parser.add_argument('--no-push', action='store_true', help='Skip GitHub push')
    args = parser.parse_args()
    
    print("=" * 70)
    print(f"ğŸš€ Starting Daily News Update - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“… Scraping news from past {args.days} days")
    print("=" * 70)
    
    all_articles = []
    stats = {
        'timestamp': datetime.now().isoformat(),
        'days_back': args.days,
        'sources': {}
    }
    
    # 1. çˆ¬å– CNA æ–°è
    print("\n[1/4] çˆ¬å–ä¸­å¤®ç¤¾æ–°è...")
    try:
        with CNAScraper(delay=1.0) as cna:
            cna_articles = cna.run(days_back=args.days)
            all_articles.extend(cna_articles)
            stats['sources']['cna'] = {
                'scraped': len(cna_articles),
                'status': 'success'
            }
            print(f"âœ“ CNA: {len(cna_articles)} ç¯‡æ–°è")
    except Exception as e:
        print(f"âœ— CNA Error: {e}")
        stats['sources']['cna'] = {'status': 'failed', 'error': str(e)}
    
    # 2. çˆ¬å–æ–°è¯ç¤¾æ–°èï¼ˆå¦‚æœæœ‰ï¼‰
    # print("\n[2/4] çˆ¬å–æ–°è¯ç¤¾æ–°è...")
    # try:
    #     with XinhuaScraper(delay=1.0) as xinhua:
    #         xinhua_articles = xinhua.run(days_back=args.days)
    #         all_articles.extend(xinhua_articles)
    #         stats['sources']['xinhua'] = {
    #             'scraped': len(xinhua_articles),
    #             'status': 'success'
    #         }
    #         print(f"âœ“ Xinhua: {len(xinhua_articles)} ç¯‡æ–°è")
    # except Exception as e:
    #     print(f"âœ— Xinhua Error: {e}")
    #     stats['sources']['xinhua'] = {'status': 'failed', 'error': str(e)}
    
    if not all_articles:
        print("\nâŒ No articles scraped. Exiting.")
        sys.exit(1)
    
    print(f"\nğŸ“Š Total articles scraped: {len(all_articles)}")
    
    # 3. ä½¿ç”¨ Grok åˆ†é¡
    print("\n[2/4] ä½¿ç”¨ Grok é€²è¡Œæ–°èåˆ†é¡...")
    import os
    api_key = os.environ.get('GROK_API_KEY')
    
    if not api_key:
        print("âŒ GROK_API_KEY not found in environment")
        sys.exit(1)
    
    try:
        with GrokNewsClassifier(api_key) as classifier:
            classified = classifier.classify_batch(all_articles, delay=1.0)
            relevant = classifier.filter_relevant(classified)
            
            stats['classification'] = {
                'total': len(classified),
                'relevant': len(relevant),
                'status': 'success'
            }
            print(f"âœ“ Classified: {len(classified)} ç¯‡")
            print(f"âœ“ Relevant: {len(relevant)} ç¯‡")
    except Exception as e:
        print(f"âœ— Classification Error: {e}")
        stats['classification'] = {'status': 'failed', 'error': str(e)}
        sys.exit(1)
    
    # 4. ä¿å­˜çµæœ
    print("\n[3/4] ä¿å­˜æ•¸æ“š...")
    data_dir = Path('data')
    data_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜åˆ†é¡çµæœ
    output_file = data_dir / 'news_classified.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(classified, f, ensure_ascii=False, indent=2)
    print(f"âœ“ Saved: {output_file}")
    
    # ä¿å­˜ç›¸é—œæ–°è
    relevant_file = data_dir / 'news_relevant.json'
    with open(relevant_file, 'w', encoding='utf-8') as f:
        json.dump(relevant, f, ensure_ascii=False, indent=2)
    print(f"âœ“ Saved: {relevant_file}")
    
    # 5. æ¨é€åˆ° GitHub
    if not args.no_push:
        print("\n[4/4] æ¨é€åˆ° GitHub...")
        try:
            updater = GitHubUpdater()
            updater.configure_git(name="PLA Data Bot", email="bot@example.com")
            
            # å‰µå»ºæ‘˜è¦æ—¥èªŒ
            updater.create_summary_log(stats, 'data/last_update.json')
            
            # æ¨é€æ•¸æ“šæ–‡ä»¶
            data_files = [
                'data/news_classified.json',
                'data/news_relevant.json',
                'data/last_update.json'
            ]
            
            success = updater.commit_and_push_data(
                data_files=data_files,
                message=f"ğŸ¤– Auto-update: {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            )
            
            if success:
                print("âœ“ Pushed to GitHub")
            else:
                print("âš ï¸  Push failed or no changes")
        except Exception as e:
            print(f"âœ— GitHub Error: {e}")
            stats['github_push'] = {'status': 'failed', 'error': str(e)}
    else:
        print("\n[4/4] Skipping GitHub push (--no-push flag)")
    
    print("\n" + "=" * 70)
    print("âœ… Update completed successfully!")
    print("=" * 70)


if __name__ == '__main__':
    main()
