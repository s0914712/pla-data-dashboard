#!/usr/bin/env python3
import re
import httpx
import time
import random
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import quote
from .base_scraper import BaseScraper

class CNAScraper(BaseScraper):
    """
    ä¸­å¤®ç¤¾è»äº‹æ–°èçˆ¬èŸ²ï¼ˆå« SerpAPI å‚™æ´ï¼‰
    ç•¶ CNA ç¶²ç«™é™æµæ™‚ï¼Œè‡ªå‹•åˆ‡æ›åˆ° Google News API
    """
    
    BASE_URL = "https://www.cna.com.tw"
    SEARCH_URL = "https://www.cna.com.tw/search/hysearchws.aspx"
    SERPAPI_URL = "https://serpapi.com/search.json"
    
    # æ¸›å°‘é—œéµå­—æ•¸é‡
    KEYWORDS = ["å…±è»", "æ±éƒ¨æˆ°å€", "å°æµ·"]
    
    # å¸¸æ…‹æ€§åˆ—è¡¨é é¢
    LIST_URLS = [
        "https://www.cna.com.tw/list/acn.aspx",  # å…©å²¸
        "https://www.cna.com.tw/list/aipl.aspx", # æ”¿æ²»
    ]
    
    CNA_HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Connection": "keep-alive",
        "Referer": "https://www.cna.com.tw/",
        "Cache-Control": "max-age=0",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
    }

    def __init__(self, timeout: int = 30, delay: float = 3.0):
        super().__init__(name="cna", timeout=timeout, delay=delay)
        self.client.headers.update(self.CNA_HEADERS)
        self.request_count = 0
        self.debug = True
        self.serpapi_key = os.environ.get('SERPAPI_KEY', '')
        self.use_serpapi_fallback = bool(self.serpapi_key)
        self.serpapi_used = False
        
        if self.use_serpapi_fallback:
            print(f"[{self.name}] âœ“ SerpAPI å‚™æ´å·²å•Ÿç”¨")
        else:
            print(f"[{self.name}] âš ï¸  æœªè¨­å®š SERPAPI_KEYï¼Œç„¡æ³•ä½¿ç”¨å‚™æ´")

    def _search_with_serpapi(self, query: str, days_back: int = 7) -> List[Dict]:
        """
        ä½¿ç”¨ SerpAPI Google News æœå°‹
        
        Args:
            query: æœå°‹é—œéµå­—
            days_back: å¤©æ•¸ç¯„åœ
            
        Returns:
            æ–°èåˆ—è¡¨
        """
        if not self.serpapi_key:
            print(f"[{self.name}] âœ— SerpAPI key æœªè¨­å®š")
            return []
        
        print(f"[{self.name}] ğŸ”„ ä½¿ç”¨ SerpAPI æœå°‹: {query}")
        
        try:
            # æ§‹å»ºæœå°‹åƒæ•¸
            params = {
                'engine': 'google_news',
                'q': f'{query} site:cna.com.tw',  # é™å®šä¸­å¤®ç¤¾ç¶²ç«™
                'api_key': self.serpapi_key,
                'gl': 'tw',  # å°ç£
                'hl': 'zh-TW',  # ç¹é«”ä¸­æ–‡
                'num': 20  # è¿”å› 20 ç­†çµæœ
            }
            
            response = httpx.get(self.SERPAPI_URL, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            articles = []
            news_results = data.get('news_results', [])
            
            print(f"[{self.name}] ğŸ“¥ SerpAPI è¿”å› {len(news_results)} ç­†çµæœ")
            
            for item in news_results:
                # æå–åŸºæœ¬è³‡è¨Š
                title = item.get('title', '')
                link = item.get('link', '')
                
                # åªè™•ç† CNA çš„é€£çµ
                if 'cna.com.tw' not in link:
                    continue
                
                # æå–æ—¥æœŸ
                date_str = self._extract_date_from_url(link)
                if not date_str:
                    # å˜—è©¦å¾ iso_date è§£æ
                    iso_date = item.get('date', {}).get('iso_date', '') if isinstance(item.get('date'), dict) else item.get('iso_date', '')
                    if iso_date:
                        try:
                            date_obj = datetime.fromisoformat(iso_date.replace('Z', '+00:00'))
                            date_str = date_obj.strftime('%Y-%m-%d')
                        except:
                            continue
                
                # æª¢æŸ¥æ—¥æœŸç¯„åœ
                date_obj = self.parse_date(date_str)
                if not date_obj or not self.is_within_days(date_obj, days_back):
                    continue
                
                articles.append({
                    'title': title.strip(),
                    'url': link,
                    'date': date_str,
                    'source_method': 'serpapi'
                })
            
            print(f"[{self.name}] âœ“ SerpAPI æ‰¾åˆ° {len(articles)} ç¯‡ç¬¦åˆæ—¥æœŸçš„æ–°è")
            self.serpapi_used = True
            return articles
            
        except Exception as e:
            print(f"[{self.name}] âœ— SerpAPI éŒ¯èª¤: {e}")
            return []

    def fetch_page(self, url: str, retries: int = 3) -> Optional[str]:
        """
        ç²å–ç¶²é å…§å®¹ï¼ˆå¢å¼·ç‰ˆï¼Œå« 429 åµæ¸¬ï¼‰
        """
        for attempt in range(retries):
            try:
                self.request_count += 1
                
                # æ¯ 5 å€‹è«‹æ±‚å¾ŒåŠ é•·å»¶é²
                if self.request_count % 5 == 0:
                    extra_delay = random.uniform(2, 5)
                    if self.debug:
                        print(f"[{self.name}] ğŸ“Š å·²ç™¼é€ {self.request_count} å€‹è«‹æ±‚ï¼Œé¡å¤–å»¶é² {extra_delay:.1f}s")
                    time.sleep(extra_delay)
                
                # åŸºæœ¬å»¶é² + éš¨æ©ŸæŠ–å‹•
                delay = self.delay + random.uniform(0, 1)
                time.sleep(delay)
                
                if self.debug:
                    print(f"[{self.name}] ğŸŒ è«‹æ±‚ #{self.request_count}: {url[:80]}...")
                
                response = self.client.get(url)
                
                # æª¢æŸ¥ç‹€æ…‹ç¢¼
                if self.debug:
                    print(f"[{self.name}] ğŸ“¥ ç‹€æ…‹ç¢¼: {response.status_code}, å…§å®¹é•·åº¦: {len(response.text)} å­—å…ƒ")
                
                # 429 é™æµåµæ¸¬
                if response.status_code == 429:
                    print(f"[{self.name}] âš ï¸  è¢«é™æµ (429)ï¼")
                    if attempt < retries - 1:
                        wait_time = (2 ** attempt) * 10
                        print(f"[{self.name}] â° ç­‰å¾… {wait_time}s å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"[{self.name}] âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œå°‡å•Ÿç”¨ SerpAPI å‚™æ´")
                        return None  # è¿”å› None è§¸ç™¼å‚™æ´
                
                response.raise_for_status()
                
                # æª¢æŸ¥å…§å®¹æ˜¯å¦æœ‰æ•ˆ
                if len(response.text) < 100:
                    print(f"[{self.name}] âš ï¸  å›æ‡‰å…§å®¹éçŸ­ ({len(response.text)} å­—å…ƒ)")
                    if self.debug:
                        print(f"[{self.name}] ğŸ“„ å…§å®¹é è¦½: {response.text[:200]}")
                
                return response.text
                
            except httpx.HTTPStatusError as e:
                print(f"[{self.name}] âŒ Attempt {attempt + 1}/{retries} - HTTP {e.response.status_code}")
                if self.debug and hasattr(e.response, 'text'):
                    print(f"[{self.name}] ğŸ“„ éŒ¯èª¤å›æ‡‰: {e.response.text[:300]}")
                
                if attempt < retries - 1:
                    wait_time = (2 ** attempt) * 5
                    time.sleep(wait_time)
            
            except Exception as e:
                print(f"[{self.name}] âŒ Attempt {attempt + 1}/{retries} - Error: {type(e).__name__}: {e}")
                if attempt < retries - 1:
                    time.sleep(self.delay * (attempt + 1))
        
        return None

    def _extract_date_from_url(self, url: str) -> str:
        """å¾ URL æå–æ—¥æœŸå­—ä¸² (YYYY-MM-DD)"""
        match = re.search(r'/(\d{8})\d+\.aspx', url)
        if match:
            d = match.group(1)
            return f"{d[:4]}-{d[4:6]}-{d[6:8]}"
        return ""

    def parse_page_items(self, content: str, source_type: str = "unknown") -> List[Dict]:
        """è§£ææœå°‹çµæœæˆ–åˆ—è¡¨é é¢"""
        if self.debug:
            print(f"[{self.name}] ğŸ” è§£æ {source_type} é é¢ ({len(content)} å­—å…ƒ)...")
        
        articles = []
        seen_urls = set()

        # æ¨¡å¼ 1: æœå°‹çµæœå°ˆç”¨çµæ§‹
        pattern_search = r'<a[^>]+href=["\']([/]?news/[a-z]+/\d+\.aspx)["\'][^>]*>.*?<div[^>]+class=["\']listInfo["\'][^>]*>.*?<h2[^>]*>([^<]+)</h2>'
        
        # æ¨¡å¼ 2: åˆ—è¡¨é é€šç”¨çµæ§‹
        pattern_list = r'<a[^>]+href=["\']([/]?news/[a-z]+/\d+\.aspx)["\'][^>]*>\s*<h2[^>]*>([^<]+)</h2>\s*</a>'

        for pattern_name, pattern in [("æœå°‹çµæ§‹", pattern_search), ("åˆ—è¡¨çµæ§‹", pattern_list)]:
            matches = re.findall(pattern, content, re.DOTALL)
            
            if self.debug and matches:
                print(f"[{self.name}] âœ“ {pattern_name} æ‰¾åˆ° {len(matches)} å€‹åŒ¹é…")
            
            for url_part, title in matches:
                full_url = url_part if url_part.startswith('http') else f"{self.BASE_URL}{url_part}"
                if full_url not in seen_urls and len(title.strip()) >= 5:
                    seen_urls.add(full_url)
                    articles.append({
                        'title': title.strip(),
                        'url': full_url,
                        'date': self._extract_date_from_url(full_url),
                        'source_method': 'direct'
                    })
        
        if self.debug:
            print(f"[{self.name}] ğŸ“‹ è§£æçµæœ: {len(articles)} ç¯‡æ–‡ç« ")
        
        return articles

    def scrape_full_content(self, url: str) -> str:
        """çˆ¬å–ä¸¦è§£æå–®ç¯‡æ–‡ç« æ­£æ–‡"""
        html = self.fetch_page(url)
        if not html:
            return ""
        
        # é–å®šå…§æ–‡å®¹å™¨
        paragraph_match = re.search(r'class="paragraph"[^>]*>(.*?)</div>', html, re.DOTALL)
        content = paragraph_match.group(1) if paragraph_match else ""
        
        if not content:
            article_match = re.search(r'<article[^>]*>(.*?)</article>', html, re.DOTALL)
            content = article_match.group(1) if article_match else ""

        # æ¸…ç†æ¨™ç±¤èˆ‡å¤šé¤˜ç©ºæ ¼
        content = re.sub(r'<[^>]+>', ' ', content)
        content = re.sub(r'\s+', ' ', content)
        
        if self.debug and content:
            print(f"[{self.name}] ğŸ“„ å…§æ–‡é•·åº¦: {len(content)} å­—å…ƒ")
        
        return content.strip()

    def run(self, days_back: int = 7, use_search: bool = False) -> List[Dict]:
        """
        åŸ·è¡Œçˆ¬å–ä¸»æµç¨‹ï¼ˆå« SerpAPI å‚™æ´ï¼‰
        
        Args:
            days_back: è¿½è¹¤å¤©æ•¸
            use_search: æ˜¯å¦ä½¿ç”¨é—œéµå­—æœå°‹
        """
        print(f"[{self.name}] ğŸš€ é–‹å§‹ä»»å‹™ï¼Œè¿½è¹¤éå» {days_back} å¤©å…§å®¹")
        print(f"[{self.name}] âš™ï¸  ä½¿ç”¨é—œéµå­—æœå°‹: {'æ˜¯' if use_search else 'å¦'}")
        print(f"[{self.name}] â±ï¸  è«‹æ±‚å»¶é²: {self.delay}s")
        
        raw_articles = []
        collected_urls = set()
        direct_fetch_failed = False

        # 1. å„ªå…ˆè™•ç†åˆ†é¡åˆ—è¡¨é 
        print(f"\n[{self.name}] ğŸ“° è™•ç†åˆ†é¡åˆ—è¡¨é ...")
        for i, list_url in enumerate(self.LIST_URLS, 1):
            print(f"[{self.name}] [{i}/{len(self.LIST_URLS)}] {list_url}")
            html = self.fetch_page(list_url)
            
            if html:
                items = self.parse_page_items(html, source_type=f"åˆ—è¡¨é  {i}")
                for item in items:
                    date_obj = self.parse_date(item['date'])
                    if date_obj and self.is_within_days(date_obj, days_back):
                        if item['url'] not in collected_urls:
                            collected_urls.add(item['url'])
                            raw_articles.append(item)
                print(f"[{self.name}] âœ“ ç´¯è¨ˆæ”¶é›†: {len(raw_articles)} ç¯‡")
            else:
                print(f"[{self.name}] âœ— ç„¡æ³•ç²å–åˆ—è¡¨é ")
                direct_fetch_failed = True

        # 2. å¯é¸ï¼šè™•ç†æœå°‹é—œéµå­—
        if use_search and not direct_fetch_failed:
            print(f"\n[{self.name}] ğŸ” è™•ç†é—œéµå­—æœå°‹...")
            for i, kw in enumerate(self.KEYWORDS, 1):
                search_url = f"{self.SEARCH_URL}?q={quote(kw)}"
                print(f"[{self.name}] [{i}/{len(self.KEYWORDS)}] æœå°‹: {kw}")
                
                html = self.fetch_page(search_url)
                if html:
                    items = self.parse_page_items(html, source_type=f"æœå°‹:{kw}")
                    for item in items:
                        date_obj = self.parse_date(item['date'])
                        if date_obj and self.is_within_days(date_obj, days_back):
                            if item['url'] not in collected_urls:
                                collected_urls.add(item['url'])
                                raw_articles.append(item)
                    print(f"[{self.name}] âœ“ ç´¯è¨ˆæ”¶é›†: {len(raw_articles)} ç¯‡")
                else:
                    print(f"[{self.name}] âœ— æœå°‹å¤±æ•—: {kw}")
                    direct_fetch_failed = True
                    break

        # 3. å•Ÿç”¨ SerpAPI å‚™æ´ï¼ˆå¦‚æœç›´æ¥æŠ“å–å¤±æ•—æˆ–çµæœå¤ªå°‘ï¼‰
        if (direct_fetch_failed or len(raw_articles) < 5) and self.use_serpapi_fallback:
            print(f"\n[{self.name}] ğŸ”„ å•Ÿç”¨ SerpAPI å‚™æ´...")
            
            for kw in self.KEYWORDS:
                serpapi_articles = self._search_with_serpapi(kw, days_back)
                for article in serpapi_articles:
                    if article['url'] not in collected_urls:
                        collected_urls.add(article['url'])
                        raw_articles.append(article)
            
            print(f"[{self.name}] âœ“ SerpAPI å‚™æ´å®Œæˆï¼Œç¸½è¨ˆ: {len(raw_articles)} ç¯‡")

        if not raw_articles:
            print(f"\n[{self.name}] âŒ æœªæ‰¾åˆ°ä»»ä½•æ–°è")
            return []

        # 4. çˆ¬å–å…§æ–‡
        print(f"\n[{self.name}] ğŸ“¥ é–‹å§‹çˆ¬å– {len(raw_articles)} ç¯‡æ–‡ç« å…§æ–‡...")
        for i, article in enumerate(raw_articles, 1):
            print(f"[{self.name}] [{i}/{len(raw_articles)}] {article['title'][:50]}...")
            article['content'] = self.scrape_full_content(article['url'])
            # ç§»é™¤å…§éƒ¨æ¨™è¨˜
            article.pop('source_method', None)

        # 5. è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
        standardized = self.to_standard_format(raw_articles)
        
        print(f"\n[{self.name}] âœ… å®Œæˆï¼æˆåŠŸçˆ¬å– {len(standardized)} ç¯‡æ–°è")
        print(f"[{self.name}] ğŸ“Š ç¸½è«‹æ±‚æ•¸: {self.request_count}")
        if self.serpapi_used:
            print(f"[{self.name}] ğŸ”„ å·²ä½¿ç”¨ SerpAPI å‚™æ´")
        
        return standardized


if __name__ == "__main__":
    print("=" * 70)
    print("CNA Scraper æ¸¬è©¦æ¨¡å¼ï¼ˆå« SerpAPI å‚™æ´ï¼‰")
    print("=" * 70)
    
    with CNAScraper(delay=3.0) as scraper:
        # åªä½¿ç”¨åˆ—è¡¨é ï¼Œä¸ç”¨æœå°‹ï¼ˆé¿å… 429ï¼‰
        results = scraper.run(days_back=7, use_search=False)
        
        print(f"\n{'='*70}")
        print(f"ç¸½è¨ˆçˆ¬å–: {len(results)} ç¯‡æ–°è")
        print(f"{'='*70}\n")
        
        for i, news in enumerate(results[:5], 1):
            print(f"{i}. [{news['date']}] {news['title']}")
            print(f"   ä¾†æº: {news['source']}")
            print(f"   å…§æ–‡: {len(news['content'])} å­—å…ƒ")
            print()
