
"""
===============================================================================
CNA æ–°èçˆ¬èŸ² (SerpAPI ç‰ˆæœ¬)
===============================================================================
ä½¿ç”¨ Google News API (via SerpAPI) ä¾†é¿é–‹ CNA ç¶²ç«™çš„åçˆ¬èŸ²æ©Ÿåˆ¶
"""

import httpx
import time
import os
import re
from datetime import datetime
from typing import List, Dict, Optional
from .base_scraper import BaseScraper


class CNAScraper(BaseScraper):
    """
    ä¸­å¤®ç¤¾è»äº‹æ–°èçˆ¬èŸ² (SerpAPI ç‰ˆæœ¬)
    å®Œå…¨ä½¿ç”¨ SerpAPI Google Newsï¼Œé¿å… 403/429 éŒ¯èª¤
    """
    
    BASE_URL = "https://www.cna.com.tw"
    SERPAPI_URL = "https://serpapi.com/search.json"
    
    # æœå°‹é—œéµå­—
    KEYWORDS = [
        "å…±è» site:cna.com.tw",
        "æ±éƒ¨æˆ°å€ site:cna.com.tw",
        "å°æµ· site:cna.com.tw",
        "è§£æ”¾è» site:cna.com.tw",
        "åœ‹é˜²éƒ¨ å…±æ©Ÿ site:cna.com.tw"
    ]

    def __init__(self, timeout: int = 30, delay: float = 1.5):
        super().__init__(name="cna", timeout=timeout, delay=delay)
        self.serpapi_key = os.environ.get('SERPAPI_KEY', '')
        
        if not self.serpapi_key:
            raise ValueError("âŒ å¿…é ˆè¨­å®š SERPAPI_KEY ç’°å¢ƒè®Šæ•¸")
        
        print(f"[{self.name}] âœ“ SerpAPI å·²å•Ÿç”¨")
        self.article_client = httpx.Client(
            timeout=timeout,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/html",
                "Accept-Language": "zh-TW,zh;q=0.9",
            },
            follow_redirects=True
        )

    def _extract_date_from_url(self, url: str) -> str:
        """å¾ URL æå–æ—¥æœŸå­—ä¸² (YYYY-MM-DD)"""
        match = re.search(r'/(\d{8})\d+\.aspx', url)
        if match:
            d = match.group(1)
            return f"{d[:4]}-{d[4:6]}-{d[6:8]}"
        return ""

    def _search_with_serpapi(self, query: str, days_back: int = 7) -> List[Dict]:
        """
        ä½¿ç”¨ SerpAPI Google News æœå°‹
        
        Args:
            query: æœå°‹é—œéµå­— (å·²åŒ…å« site:cna.com.tw)
            days_back: å¤©æ•¸ç¯„åœ
            
        Returns:
            æ–°èåˆ—è¡¨
        """
        print(f"[{self.name}] ğŸ” SerpAPI æœå°‹: {query}")
        
        try:
            params = {
                'engine': 'google_news',
                'q': query,
                'api_key': self.serpapi_key,
                'gl': 'tw',  # å°ç£
                'hl': 'zh-TW',  # ç¹é«”ä¸­æ–‡
                'num': 30  # æ¯å€‹é—œéµå­—è¿”å› 30 ç­†
            }
            
            response = httpx.get(self.SERPAPI_URL, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            articles = []
            news_results = data.get('news_results', [])
            
            print(f"[{self.name}] ğŸ“¥ è¿”å› {len(news_results)} ç­†çµæœ")
            
            for item in news_results:
                # æå–åŸºæœ¬è³‡è¨Š
                title = item.get('title', '')
                link = item.get('link', '')
                
                # åªè™•ç† CNA çš„é€£çµ
                if 'cna.com.tw' not in link:
                    continue
                
                # æå–æ—¥æœŸ
                date_str = self._extract_date_from_url(link)
                
                # å¦‚æœ URL æ²’æœ‰æ—¥æœŸï¼Œå˜—è©¦å¾ API å›å‚³çš„æ™‚é–“è§£æ
                if not date_str:
                    # å˜—è©¦å¤šç¨®æ—¥æœŸæ ¼å¼
                    date_value = item.get('date', '')
                    
                    # æ ¼å¼1: å­—å…¸æ ¼å¼ {'iso_date': '...'}
                    if isinstance(date_value, dict):
                        iso_date = date_value.get('iso_date', '')
                        if iso_date:
                            try:
                                date_obj = datetime.fromisoformat(iso_date.replace('Z', '+00:00'))
                                date_str = date_obj.strftime('%Y-%m-%d')
                            except:
                                pass
                    # æ ¼å¼2: ç›´æ¥æ˜¯ ISO å­—ç¬¦ä¸²
                    elif isinstance(date_value, str) and date_value:
                        try:
                            date_obj = datetime.fromisoformat(date_value.replace('Z', '+00:00'))
                            date_str = date_obj.strftime('%Y-%m-%d')
                        except:
                            pass
                    
                    # å¦‚æœé‚„æ˜¯æ²’æœ‰ï¼Œè·³é
                    if not date_str:
                        continue
                
                # æª¢æŸ¥æ—¥æœŸç¯„åœ
                date_obj = self.parse_date(date_str)
                if not date_obj or not self.is_within_days(date_obj, days_back):
                    continue
                
                articles.append({
                    'title': title.strip(),
                    'url': link,
                    'date': date_str
                })
            
            print(f"[{self.name}] âœ“ æ‰¾åˆ° {len(articles)} ç¯‡ç¬¦åˆæ—¥æœŸçš„æ–°è")
            time.sleep(self.delay)  # é¿å… API é™æµ
            return articles
            
        except Exception as e:
            print(f"[{self.name}] âœ— SerpAPI éŒ¯èª¤: {e}")
            return []

    def scrape_full_content(self, url: str) -> str:
        """
        çˆ¬å–ä¸¦è§£æå–®ç¯‡æ–‡ç« æ­£æ–‡
        ä½¿ç”¨æ›´å¯¬é¬†çš„éŒ¯èª¤è™•ç†
        """
        try:
            time.sleep(self.delay)
            response = self.article_client.get(url)
            
            # å³ä½¿ 403 ä¹Ÿå˜—è©¦è§£æï¼ˆæœ‰äº›å…§å®¹å¯èƒ½åœ¨éŒ¯èª¤é é¢ï¼‰
            if response.status_code == 403:
                print(f"[{self.name}] âš ï¸  {url} è¿”å› 403ï¼Œå˜—è©¦æå–æ¨™é¡Œ")
                return f"[ç„¡æ³•ç²å–å®Œæ•´å…§æ–‡ï¼Œå¯èƒ½éœ€è¦ç€è¦½å™¨è¨ªå•]"
            
            response.raise_for_status()
            html = response.text
            
            # é–å®šå…§æ–‡å®¹å™¨
            paragraph_match = re.search(r'class="paragraph"[^>]*>(.*?)</div>', html, re.DOTALL)
            content = paragraph_match.group(1) if paragraph_match else ""
            
            if not content:
                article_match = re.search(r'<article[^>]*>(.*?)</article>', html, re.DOTALL)
                content = article_match.group(1) if article_match else ""
            
            # æ¸…ç†æ¨™ç±¤èˆ‡å¤šé¤˜ç©ºæ ¼
            content = re.sub(r'<[^>]+>', ' ', content)
            content = re.sub(r'\s+', ' ', content)
            
            return content.strip() if content else "[å…§æ–‡æå–å¤±æ•—]"
            
        except Exception as e:
            print(f"[{self.name}] âœ— å…§æ–‡æŠ“å–éŒ¯èª¤ ({url}): {e}")
            return "[å…§æ–‡æå–å¤±æ•—]"

    def run(self, days_back: int = 7) -> List[Dict]:
        """
        åŸ·è¡Œçˆ¬å–ä¸»æµç¨‹ï¼ˆå®Œå…¨ä½¿ç”¨ SerpAPIï¼‰
        
        Args:
            days_back: è¿½è¹¤å¤©æ•¸
            
        Returns:
            æ¨™æº–æ ¼å¼æ–°èåˆ—è¡¨
        """
        print(f"[{self.name}] ğŸš€ é–‹å§‹ä»»å‹™ (SerpAPI æ¨¡å¼)")
        print(f"[{self.name}] ğŸ“… è¿½è¹¤éå» {days_back} å¤©å…§å®¹")
        print(f"[{self.name}] ğŸ”‘ ä½¿ç”¨ {len(self.KEYWORDS)} å€‹æœå°‹é—œéµå­—")
        
        raw_articles = []
        collected_urls = set()

        # ä½¿ç”¨ SerpAPI æœå°‹æ‰€æœ‰é—œéµå­—
        for i, query in enumerate(self.KEYWORDS, 1):
            print(f"\n[{self.name}] [{i}/{len(self.KEYWORDS)}] è™•ç†é—œéµå­—...")
            articles = self._search_with_serpapi(query, days_back)
            
            for article in articles:
                if article['url'] not in collected_urls:
                    collected_urls.add(article['url'])
                    raw_articles.append(article)
            
            print(f"[{self.name}] âœ“ ç´¯è¨ˆæ”¶é›†: {len(raw_articles)} ç¯‡ï¼ˆå»é‡å¾Œï¼‰")

        if not raw_articles:
            print(f"\n[{self.name}] âŒ æœªæ‰¾åˆ°ä»»ä½•æ–°è")
            return []

        # çˆ¬å–å…§æ–‡
        print(f"\n[{self.name}] ğŸ“¥ é–‹å§‹çˆ¬å– {len(raw_articles)} ç¯‡æ–‡ç« å…§æ–‡...")
        success_count = 0
        
        for i, article in enumerate(raw_articles, 1):
            print(f"[{self.name}] [{i}/{len(raw_articles)}] {article['title'][:50]}...")
            article['content'] = self.scrape_full_content(article['url'])
            
            if article['content'] and "[å…§æ–‡æå–å¤±æ•—]" not in article['content']:
                success_count += 1

        # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
        standardized = self.to_standard_format(raw_articles)
        
        print(f"\n[{self.name}] âœ… å®Œæˆï¼")
        print(f"[{self.name}] ğŸ“Š ç¸½è¨ˆ: {len(standardized)} ç¯‡æ–°è")
        print(f"[{self.name}] ğŸ“„ å…§æ–‡æˆåŠŸ: {success_count}/{len(raw_articles)} ç¯‡")
        
        return standardized

    def close(self):
        """é—œé–‰é€£æ¥"""
        super().close()
        self.article_client.close()


if __name__ == "__main__":
    import sys
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    if not os.environ.get('SERPAPI_KEY'):
        print("âŒ éŒ¯èª¤: è«‹è¨­å®š SERPAPI_KEY ç’°å¢ƒè®Šæ•¸")
        print("   export SERPAPI_KEY='your_api_key'")
        sys.exit(1)
    
    print("=" * 70)
    print("CNA Scraper æ¸¬è©¦æ¨¡å¼ (SerpAPI)")
    print("=" * 70)
    
    try:
        with CNAScraper(delay=1.5) as scraper:
            results = scraper.run(days_back=3)
            
            print(f"\n{'='*70}")
            print(f"ç¸½è¨ˆçˆ¬å–: {len(results)} ç¯‡æ–°è")
            print(f"{'='*70}\n")
            
            for i, news in enumerate(results[:5], 1):
                print(f"{i}. [{news['date']}] {news['title']}")
                print(f"   ä¾†æº: {news['source']}")
                print(f"   URL: {news['url']}")
                print(f"   å…§æ–‡é•·åº¦: {len(news.get('content', ''))} å­—å…ƒ")
                print()
    
    except ValueError as e:
        print(f"\n{e}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
