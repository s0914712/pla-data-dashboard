#!/usr/bin/env python3
"""
===============================================================================
ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Šçˆ¬èŸ² (è»æ¼”å°ˆç”¨ç‰ˆ - å«è©³ç´°å…§å®¹)
MSA Navigation Warning Scraper (Military Focus - Full Content)
===============================================================================

ç›®æ¨™: çˆ¬å–è»äº‹æ¼”ç¿’ç›¸é—œèˆªè¡Œè­¦å‘Šçš„è©³ç´°ä¿¡æ¯
åŒ…å«: ç¶“ç·¯åº¦ã€æ—¥æœŸç¯„åœã€è­¦å‘Šæ¦‚è¦
"""

import time
import re
from datetime import datetime
from typing import List, Dict, Optional
from .base_scraper import BaseScraper


class NavigationWarningScraper(BaseScraper):
    """ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Šçˆ¬èŸ²ï¼ˆè»æ¼”å°ˆç”¨ - å«è©³ç´°å…§å®¹ï¼‰"""
    
    BASE_URL = "https://www.msa.gov.cn"
    NAV_WARNING_CHANNEL = '9C219298-B27F-460E-995A-99401B3FF6AF'
    
    # ğŸ¯ è»äº‹æ¼”ç¿’é—œéµå­—
    MILITARY_KEYWORDS = [
        'å†›äº‹', 'è»äº‹', 'æ¼”ä¹ ', 'æ¼”ç¿’', 'å®å¼¹', 'å¯¦å½ˆ',
        'ç«ç‚®å°„å‡»', 'ç«ç‚®å°„æ“Š', 'å°„å‡»è®­ç»ƒ', 'å°„æ“Šè¨“ç·´',
        'ç¦èˆª', 'ç¦æ­¢é©¶å…¥', 'ç¦æ­¢é§›å…¥',
        'ç«ç®­å‘å°„', 'ç«ç®­ç™¼å°„', 'ç«ç®­æ®‹éª¸', 'ç«ç®­æ®˜éª¸',
        'MILITARY', 'EXERCISE',
        'å†›æ¼”', 'è»æ¼”', 'æ¼”è®­', 'æ¼”è¨“',
        'è”åˆæ¼”ç»ƒ', 'è¯åˆæ¼”ç·´', 'å®æˆ˜åŒ–è®­ç»ƒ', 'å¯¦æˆ°åŒ–è¨“ç·´',
        'å¯¼å¼¹è¯•å°„', 'å°å½ˆè©¦å°„', 'æ­¦å™¨è¯•éªŒ', 'æ­¦å™¨è©¦é©—',
        'æµ·ä¸Šå®å¼¹', 'æµ·ä¸Šå¯¦å½ˆ', 'ç©ºä¸­æ¼”ç»ƒ', 'ç©ºä¸­æ¼”ç·´',
        'æˆ˜å¤‡å·¡èˆª', 'æˆ°å‚™å·¡èˆª', 'å†›äº‹ç¦åŒº', 'è»äº‹ç¦å€',
        'é¶åœº', 'é¶å ´', 'å°„å‡»åœº', 'å°„æ“Šå ´',
    ]
    
    # æ’é™¤é—œéµå­—
    EXCLUDE_KEYWORDS = [
        'æ‹–å¸¦', 'æ‹–å¸¶', 'LNG', 'æ¶²åŒ–å¤©ç„¶æ°”',
        'æ–½å·¥', 'æµ·ä¸Šæ–½å·¥', 'æ¸¬é‡', 'æµ‹é‡',
        'æ‰“æ', 'æ‰“æ’ˆ', 'è½½è¿', 'è¼‰é‹',
        'å¤§ä»¶', 'è¶…å¤§ä»¶', 'åŠ æ³¨', 'è£œçµ¦',
    ]
    
    # æµ·äº‹å±€ä»£ç¢¼æ˜ å°„
    MSA_CODE_MAP = {
        'æ²ª': 'ä¸Šæµ·æµ·äº‹å±€', 'æ´¥': 'å¤©æ´¥æµ·äº‹å±€',
        'è¾½': 'è¾½å®æµ·äº‹å±€', 'å†€': 'æ²³åŒ—æµ·äº‹å±€',
        'é²': 'å±±ä¸œæµ·äº‹å±€', 'æµ™': 'æµ™æ±Ÿæµ·äº‹å±€',
        'é—½': 'ç¦å»ºæµ·äº‹å±€', 'ç²¤': 'å¹¿ä¸œæµ·äº‹å±€',
        'æ¡‚': 'å¹¿è¥¿æµ·äº‹å±€', 'ç¼': 'æµ·å—æµ·äº‹å±€',
        'æ·±': 'æ·±åœ³æµ·äº‹å±€', 'å¦': 'å¦é—¨æµ·äº‹å±€',
        'ç”¬': 'å®æ³¢æµ·äº‹å±€', 'é’': 'é’å²›æµ·äº‹å±€',
        'è¿': 'å¤§è¿æµ·äº‹å±€', 'ç ': 'ç æµ·æµ·äº‹å±€',
        'æ±•': 'æ±•å¤´æµ·äº‹å±€', 'æ¹›': 'æ¹›æ±Ÿæµ·äº‹å±€',
        'è‹': 'æ±Ÿè‹æµ·äº‹å±€', 'é•¿æ±Ÿ': 'é•¿æ±Ÿæµ·äº‹å±€',
    }
    
    def __init__(self, timeout: int = 30, delay: float = 1.0):
        super().__init__(name="msa_military", timeout=timeout, delay=delay)
    
    def is_military_exercise(self, title: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºè»äº‹æ¼”ç¿’ç›¸é—œè­¦å‘Š"""
        title_lower = title.lower()
        
        # æª¢æŸ¥æ’é™¤é—œéµå­—
        for exclude in self.EXCLUDE_KEYWORDS:
            if exclude.lower() in title_lower:
                return False
        
        # æª¢æŸ¥è»äº‹é—œéµå­—
        for keyword in self.MILITARY_KEYWORDS:
            if keyword.lower() in title_lower:
                return True
        
        return False
    
    def extract_msa_from_title(self, title: str) -> str:
        """å¾æ¨™é¡Œä¸­æå–æµ·äº‹å±€åç¨±"""
        for code, msa_name in self.MSA_CODE_MAP.items():
            if f'{code}èˆªè­¦' in title or f'{code}èˆªè¡Œè­¦å‘Š' in title:
                return msa_name
        
        for msa_name in self.MSA_CODE_MAP.values():
            if msa_name.replace('æµ·äº‹å±€', '') in title:
                return msa_name
        
        return 'æœªçŸ¥æµ·äº‹å±€'
    
    def extract_matched_keywords(self, title: str) -> List[str]:
        """æå–åŒ¹é…çš„è»äº‹é—œéµå­—"""
        matched = []
        title_lower = title.lower()
        
        for keyword in self.MILITARY_KEYWORDS:
            if keyword.lower() in title_lower:
                matched.append(keyword)
        
        return matched
    
    def parse_article_detail(self, url: str) -> Dict:
        """
        è§£ææ–‡ç« è©³ç´°å…§å®¹ï¼Œæå–ç¶“ç·¯åº¦ã€æ—¥æœŸç¯„åœã€æ¦‚è¦
        
        Returns:
            {
                'coordinates': List[Dict],  # ç¶“ç·¯åº¦åˆ—è¡¨
                'date_range': str,          # æ—¥æœŸç¯„åœ
                'summary': str              # æ¦‚è¦
            }
        """
        html = self.fetch_page(url)
        if not html:
            return {
                'coordinates': [],
                'date_range': '',
                'summary': ''
            }
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ­£æ–‡å…§å®¹
        content_div = soup.find('div', class_='content') or soup.find('div', id='content')
        if not content_div:
            # å˜—è©¦å…¶ä»–å¯èƒ½çš„å…§å®¹å€åŸŸ
            content_div = soup.find('div', class_='article-content') or soup.find('article')
        
        if not content_div:
            return {'coordinates': [], 'date_range': '', 'summary': ''}
        
        text = content_div.get_text()
        
        # ğŸ” æå–ç¶“ç·¯åº¦
        coordinates = self.extract_coordinates(text)
        
        # ğŸ” æå–æ—¥æœŸç¯„åœ
        date_range = self.extract_date_range(text)
        
        # ğŸ” æå–æ¦‚è¦ï¼ˆå‰300å­—ï¼‰
        summary = self.extract_summary(text)
        
        return {
            'coordinates': coordinates,
            'date_range': date_range,
            'summary': summary
        }
    
    def extract_coordinates(self, text: str) -> List[Dict]:
        """
        æå–ç¶“ç·¯åº¦åæ¨™
        
        æ”¯æŒæ ¼å¼:
        - 30Â°15â€²N 122Â°30â€²E
        - 30-15N 122-30E
        - 30Â°15.5â€²N 122Â°30.5â€²E (å«å°æ•¸)
        """
        coordinates = []
        
        # æ¨¡å¼1: åº¦åˆ†ç§’æ ¼å¼ (30Â°15â€²N 122Â°30â€²E)
        pattern1 = r'(\d+)Â°(\d+)(?:â€²|åˆ†)([NSåŒ—å—])\s+(\d+)Â°(\d+)(?:â€²|åˆ†)([EWæ±è¥¿])'
        matches1 = re.findall(pattern1, text)
        
        for match in matches1:
            lat_deg, lat_min, lat_dir, lon_deg, lon_min, lon_dir = match
            
            # è½‰æ›ç‚ºåé€²åˆ¶
            lat = float(lat_deg) + float(lat_min) / 60
            lon = float(lon_deg) + float(lon_min) / 60
            
            if lat_dir in ['S', 'å—']:
                lat = -lat
            if lon_dir in ['W', 'è¥¿']:
                lon = -lon
            
            coordinates.append({
                'lat': round(lat, 4),
                'lon': round(lon, 4),
                'original': f"{lat_deg}Â°{lat_min}â€²{lat_dir} {lon_deg}Â°{lon_min}â€²{lon_dir}"
            })
        
        # æ¨¡å¼2: ç°¡åŒ–æ ¼å¼ (30-15N 122-30E)
        pattern2 = r'(\d+)-(\d+)([NSåŒ—å—])\s+(\d+)-(\d+)([EWæ±è¥¿])'
        matches2 = re.findall(pattern2, text)
        
        for match in matches2:
            lat_deg, lat_min, lat_dir, lon_deg, lon_min, lon_dir = match
            
            lat = float(lat_deg) + float(lat_min) / 60
            lon = float(lon_deg) + float(lon_min) / 60
            
            if lat_dir in ['S', 'å—']:
                lat = -lat
            if lon_dir in ['W', 'è¥¿']:
                lon = -lon
            
            coordinates.append({
                'lat': round(lat, 4),
                'lon': round(lon, 4),
                'original': f"{lat_deg}-{lat_min}{lat_dir} {lon_deg}-{lon_min}{lon_dir}"
            })
        
        # æ¨¡å¼3: åº¦åˆ†ç§’.å°æ•¸æ ¼å¼ (30Â°15.5â€²N)
        pattern3 = r'(\d+)Â°([\d.]+)(?:â€²|åˆ†)([NSåŒ—å—])\s+(\d+)Â°([\d.]+)(?:â€²|åˆ†)([EWæ±è¥¿])'
        matches3 = re.findall(pattern3, text)
        
        for match in matches3:
            lat_deg, lat_min, lat_dir, lon_deg, lon_min, lon_dir = match
            
            lat = float(lat_deg) + float(lat_min) / 60
            lon = float(lon_deg) + float(lon_min) / 60
            
            if lat_dir in ['S', 'å—']:
                lat = -lat
            if lon_dir in ['W', 'è¥¿']:
                lon = -lon
            
            coordinates.append({
                'lat': round(lat, 4),
                'lon': round(lon, 4),
                'original': f"{lat_deg}Â°{lat_min}â€²{lat_dir} {lon_deg}Â°{lon_min}â€²{lon_dir}"
            })
        
        # å»é‡
        seen = set()
        unique_coords = []
        for coord in coordinates:
            key = (coord['lat'], coord['lon'])
            if key not in seen:
                seen.add(key)
                unique_coords.append(coord)
        
        return unique_coords
    
    def extract_date_range(self, text: str) -> str:
        """
        æå–æ—¥æœŸç¯„åœ
        
        æ”¯æŒæ ¼å¼:
        - 2024å¹´1æœˆ15æ—¥è‡³1æœˆ20æ—¥
        - 1æœˆ15æ—¥è‡³20æ—¥
        - 2024-01-15è‡³2024-01-20
        """
        # æ¨¡å¼1: å®Œæ•´æ—¥æœŸç¯„åœ
        pattern1 = r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥è‡³(\d{1,2})æœˆ(\d{1,2})æ—¥'
        match1 = re.search(pattern1, text)
        if match1:
            year, m1, d1, m2, d2 = match1.groups()
            return f"{year}-{m1.zfill(2)}-{d1.zfill(2)} è‡³ {year}-{m2.zfill(2)}-{d2.zfill(2)}"
        
        # æ¨¡å¼2: åŒæœˆæ—¥æœŸç¯„åœ
        pattern2 = r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥è‡³(\d{1,2})æ—¥'
        match2 = re.search(pattern2, text)
        if match2:
            year, month, d1, d2 = match2.groups()
            return f"{year}-{month.zfill(2)}-{d1.zfill(2)} è‡³ {year}-{month.zfill(2)}-{d2.zfill(2)}"
        
        # æ¨¡å¼3: çŸ­æ ¼å¼
        pattern3 = r'(\d{1,2})æœˆ(\d{1,2})æ—¥è‡³(\d{1,2})æ—¥'
        match3 = re.search(pattern3, text)
        if match3:
            month, d1, d2 = match3.groups()
            year = datetime.now().year
            return f"{year}-{month.zfill(2)}-{d1.zfill(2)} è‡³ {year}-{month.zfill(2)}-{d2.zfill(2)}"
        
        # æ¨¡å¼4: ISO æ ¼å¼
        pattern4 = r'(\d{4}-\d{2}-\d{2})\s*è‡³\s*(\d{4}-\d{2}-\d{2})'
        match4 = re.search(pattern4, text)
        if match4:
            return f"{match4.group(1)} è‡³ {match4.group(2)}"
        
        return ''
    
    def extract_summary(self, text: str) -> str:
        """æå–æ¦‚è¦ï¼ˆæ¸…ç†å¾Œçš„å‰300å­—ï¼‰"""
        # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ
        summary = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤å¸¸è¦‹çš„ç¶²é å…ƒç´ 
        summary = re.sub(r'(é¦–é¡µ|è¿”å›|æ‰“å°|åˆ†äº«|ç›¸å…³é“¾æ¥)', '', summary)
        
        # æˆªå–å‰300å­—
        summary = summary.strip()[:300]
        
        return summary
    
    def scrape_page(self, page: int, page_size: int = 50) -> List[Dict]:
        """çˆ¬å–å–®é èˆªè¡Œè­¦å‘Šï¼ˆåªè¿”å›è»æ¼”ç›¸é—œï¼‰"""
        url = f"{self.BASE_URL}/page/channelArticles.do"
        params = {
            'channelids': self.NAV_WARNING_CHANNEL,
            'currpage': str(page),
            'pagesize': str(page_size)
        }
        
        html = self.fetch_page(
            url + '?' + '&'.join(f"{k}={v}" for k, v in params.items())
        )
        
        if not html:
            return []
        
        warnings = []
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        all_lis = soup.find_all('li')
        
        for li in all_lis:
            link = li.find('a', href=lambda x: x and 'articleId' in x)
            if not link:
                continue
            
            # æå–æ¨™é¡Œ
            title_span = link.find('span')
            title = title_span.text.strip() if title_span else link.text.strip()
            
            # ğŸ¯ åªä¿ç•™è»æ¼”ç›¸é—œ
            if not self.is_military_exercise(title):
                continue
            
            # æå–æ—¥æœŸ
            date_text = None
            for span in li.find_all('span'):
                text = span.text.strip()
                if re.match(r'\[\d{4}-\d{2}-\d{2}\]', text):
                    date_text = text.strip('[]')
                    break
            
            # æå– URL
            href = link['href']
            article_id = None
            if 'articleId=' in href:
                article_id = href.split('articleId=')[1].split('&')[0]
            
            full_url = self.BASE_URL + href if not href.startswith('http') else href
            
            # è­˜åˆ¥æµ·äº‹å±€å’Œé—œéµå­—
            msa_name = self.extract_msa_from_title(title)
            matched_keywords = self.extract_matched_keywords(title)
            
            warning = {
                'title': title,
                'msa': msa_name,
                'matched_keywords': ','.join(matched_keywords),
                'publish_date': date_text,
                'article_id': article_id,
                'url': full_url,
                'scraped_at': datetime.now().isoformat()
            }
            
            warnings.append(warning)
        
        return warnings
    
    def run(self, max_pages: int = 50, days_back: int = 365, fetch_details: bool = True) -> List[Dict]:
        """
        åŸ·è¡Œå®Œæ•´çˆ¬å–æµç¨‹
        
        Args:
            max_pages: æœ€å¤§çˆ¬å–é æ•¸
            days_back: çˆ¬å–éå»å¹¾å¤©çš„æ•¸æ“š
            fetch_details: æ˜¯å¦çˆ¬å–è©³ç´°å…§å®¹ï¼ˆç¶“ç·¯åº¦ã€æ—¥æœŸç¯„åœã€æ¦‚è¦ï¼‰
            
        Returns:
            æ¨™æº–æ ¼å¼çš„è»æ¼”è­¦å‘Šåˆ—è¡¨
        """
        print(f"[{self.name}] ğŸ¯ é–‹å§‹çˆ¬å–è»äº‹æ¼”ç¿’ç›¸é—œèˆªè¡Œè­¦å‘Š...")
        print(f"[{self.name}] ğŸ“… ç›®æ¨™: éå» {days_back} å¤©ï¼Œæœ€å¤š {max_pages} é ")
        print(f"[{self.name}] ğŸ” è©³ç´°å…§å®¹: {'æ˜¯' if fetch_details else 'å¦'}")
        
        all_warnings = []
        seen_ids = set()
        
        # ç¬¬ä¸€éšæ®µï¼šçˆ¬å–åˆ—è¡¨
        for page in range(1, max_pages + 1):
            print(f"[{self.name}] ğŸ“„ çˆ¬å–ç¬¬ {page}/{max_pages} é ...")
            
            warnings = self.scrape_page(page)
            
            if not warnings:
                if page >= 10:
                    consecutive_empty = sum(1 for p in range(max(1, page - 9), page + 1) 
                                          if not any(w.get('_page') == p for w in all_warnings))
                    if consecutive_empty >= 10:
                        print(f"[{self.name}] âš ï¸  é€£çºŒå¤šé ç„¡æ•¸æ“šï¼Œåœæ­¢çˆ¬å–")
                        break
                continue
            
            page_added = 0
            for warning in warnings:
                if warning['article_id'] in seen_ids:
                    continue
                
                date_obj = self.parse_date(warning['publish_date'])
                if not date_obj or not self.is_within_days(date_obj, days_back):
                    continue
                
                seen_ids.add(warning['article_id'])
                warning['_page'] = page
                all_warnings.append(warning)
                page_added += 1
            
            print(f"[{self.name}] âœ… æœ¬é æ–°å¢ {page_added} æ¢ï¼Œç´¯è¨ˆ {len(all_warnings)} æ¢")
        
        # ç¬¬äºŒéšæ®µï¼šçˆ¬å–è©³ç´°å…§å®¹
        if fetch_details and all_warnings:
            print(f"\n[{self.name}] ğŸ“¥ é–‹å§‹çˆ¬å– {len(all_warnings)} æ¢è­¦å‘Šçš„è©³ç´°å…§å®¹...")
            
            for i, warning in enumerate(all_warnings, 1):
                print(f"[{self.name}] [{i}/{len(all_warnings)}] {warning['title'][:40]}...")
                
                details = self.parse_article_detail(warning['url'])
                warning.update(details)
                
                # é¡¯ç¤ºæå–çµæœ
                if details['coordinates']:
                    print(f"[{self.name}]   âœ“ ç¶“ç·¯åº¦: {len(details['coordinates'])} å€‹é»")
                if details['date_range']:
                    print(f"[{self.name}]   âœ“ æ—¥æœŸç¯„åœ: {details['date_range']}")
        
        # ç§»é™¤å…§éƒ¨å­—æ®µ
        for warning in all_warnings:
            warning.pop('_page', None)
        
        print(f"\n[{self.name}] âœ… çˆ¬å–å®Œæˆï¼å…± {len(all_warnings)} æ¢è»äº‹æ¼”ç¿’è­¦å‘Š")
        
        return self.to_standard_format(all_warnings)
    
    def to_standard_format(self, warnings: List[Dict]) -> List[Dict]:
        """è½‰æ›ç‚ºæ¨™æº–æ ¼å¼"""
        standardized = []
        for warning in warnings:
            date_obj = self.parse_date(warning.get('publish_date', ''))
            
            # æ ¼å¼åŒ–ç¶“ç·¯åº¦ç‚ºå­—ç¬¦ä¸²
            coords_str = ''
            if warning.get('coordinates'):
                coords_list = [f"{c['lat']},{c['lon']}" for c in warning['coordinates']]
                coords_str = '; '.join(coords_list)
            
            std_warning = {
                'publish_date': date_obj.strftime('%Y-%m-%d') if date_obj else '',
                'title': warning.get('title', '').strip(),
                'msa': warning.get('msa', ''),
                'matched_keywords': warning.get('matched_keywords', ''),
                'date_range': warning.get('date_range', ''),
                'coordinates': coords_str,
                'summary': warning.get('summary', ''),
                'article_id': warning.get('article_id', ''),
                'url': warning.get('url', ''),
                'scraped_at': warning.get('scraped_at', '')
            }
            
            if std_warning['publish_date'] and std_warning['title']:
                standardized.append(std_warning)
        
        return standardized


def test_scraper():
    """æ¸¬è©¦çˆ¬èŸ²"""
    print("=" * 80)
    print("MSA Military Exercise Warning Scraper æ¸¬è©¦")
    print("=" * 80)
    
    with NavigationWarningScraper(delay=1.5) as scraper:
        # æ¸¬è©¦ï¼šåªçˆ¬3é ï¼Œéå»30å¤©ï¼ŒåŒ…å«è©³ç´°å…§å®¹
        warnings = scraper.run(max_pages=3, days_back=30, fetch_details=True)
        
        print(f"\nç¸½è¨ˆ: {len(warnings)} æ¢è»äº‹æ¼”ç¿’è­¦å‘Š\n")
        
        if warnings:
            print("ç¤ºä¾‹æ•¸æ“š:")
            for i, w in enumerate(warnings[:2], 1):
                print(f"\n[{i}] {w['title']}")
                print(f"    ç™¼å¸ƒæ—¥æœŸ: {w['publish_date']}")
                print(f"    æ—¥æœŸç¯„åœ: {w['date_range']}")
                print(f"    ç¶“ç·¯åº¦: {w['coordinates'][:100]}...")
                print(f"    æ¦‚è¦: {w['summary'][:100]}...")


if __name__ == '__main__':
    test_scraper()
