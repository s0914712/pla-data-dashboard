#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
===============================================================================
ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Šçˆ¬èŸ² / MSA Navigation Warning Scraper
===============================================================================

å°ˆæ³¨æ–¼è»äº‹ä»»å‹™ã€å¯¦å½ˆå°„æ“Šç­‰ç›¸é—œå…¬å‘Š
"""

import re
from datetime import datetime
from typing import List, Dict, Optional
from .base_scraper import BaseScraper


class NavigationWarningScraper(BaseScraper):
    """ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Šçˆ¬èŸ²ï¼ˆè»äº‹å°ˆç”¨ï¼‰"""
    
    BASE_URL = "https://www.msa.gov.cn"
    
    # å„æµ·äº‹å±€é »é“ ID
    CHANNELS = {
        'ä¸Šæµ·æµ·äº‹å±€': '94DF14CE-1110-415D-A44E-67593E76619F',
        'å¤©æ´¥æµ·äº‹å±€': 'BDBA5FAD-6E5D-4867-9F97-0FCF8EFB8636',
        'è¾½å®æµ·äº‹å±€': 'C8896863-B101-4C43-8705-536A03EB46FF',
        'æ²³åŒ—æµ·äº‹å±€': '93B73989-D220-45F9-BC32-70A6EBA35180',
        'å±±ä¸œæµ·äº‹å±€': '36EA3354-C8F8-4953-ABA0-82D6D989C750',
        'æµ™æ±Ÿæµ·äº‹å±€': '8E10EA74-EB9E-4C96-90F8-F891968ADD80',
        'ç¦å»ºæµ·äº‹å±€': '7B084057-6038-4570-A0FB-44E9204C4B1D',
        'å¹¿ä¸œæµ·äº‹å±€': '1E478D40-9E85-4918-BF12-478B8A19F4A8',
        'å¹¿è¥¿æµ·äº‹å±€': '86DE2FFF-FF2C-47F9-8359-FD1F20D6508F',
        'æµ·å—æµ·äº‹å±€': 'D3340711-057B-494B-8FA0-9EEDC4C5EAD9',
        'æ·±åœ³æµ·äº‹å±€': '325FDC08-92B4-4313-A63E-E5C165BE98EC',
        'è¿äº‘æ¸¯æµ·äº‹å±€': 'FA4501F3-DBE4-4F70-BC72-6F27132D4E04',
    }
    
    # è»äº‹ç›¸é—œé—œéµå­—
    MILITARY_KEYWORDS = [
        'å†›äº‹', 'æ¼”ä¹ ', 'å®å¼¹', 'ç«ç‚®å°„å‡»', 'å°„å‡»è®­ç»ƒ',
        'ç¦èˆª', 'ç¦æ­¢é©¶å…¥', 'MILITARY', 'ç«ç®­å‘å°„', 'ç«ç®­æ®‹éª¸',
        'è»äº‹', 'æ¼”ç¿’', 'å¯¦å½ˆ', 'å°„æ“Šè¨“ç·´', 'ç¦æ­¢é§›å…¥',
        'EXERCISE', 'MISSION', 'å†›æ¼”', 'è»æ¼”'
    ]
    
    def __init__(self, timeout: int = 30, delay: float = 1.0):
        super().__init__(name="msa_military", timeout=timeout, delay=delay)
    
    def is_military_related(self, title: str) -> bool:
        """æª¢æŸ¥æ¨™é¡Œæ˜¯å¦èˆ‡è»äº‹ç›¸é—œ"""
        return any(kw in title for kw in self.MILITARY_KEYWORDS)
    
    def fetch_channel_list(self, channel_id: str, channel_name: str, page: int = 1) -> List[Dict]:
        """å–å¾—ç‰¹å®šæµ·äº‹å±€çš„èˆªè­¦åˆ—è¡¨"""
        url = f"{self.BASE_URL}/page/channelArticles.do?channelids={channel_id}&pageNo={page}"
        
        html = self.fetch_page(url)
        if not html:
            print(f"[{self.name}] âŒ ç„¡æ³•è¨ªå• {channel_name}")
            return []
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        # å°‹æ‰¾æ‰€æœ‰æ–‡ç« éˆæ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            if '/page/article.do?articleId=' not in href:
                continue
            
            title = link.get_text(strip=True)
            if not title:
                continue
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºè»äº‹ç›¸é—œ
            is_military = self.is_military_related(title)
            
            # æå–æ—¥æœŸ
            date_text = ''
            next_sibling = link.find_next_sibling(string=True)
            if next_sibling:
                date_match = re.search(r'\d{4}-\d{2}-\d{2}', str(next_sibling))
                if date_match:
                    date_text = date_match.group()
            
            if not date_text:
                parent = link.find_parent('li') or link.find_parent('div')
                if parent:
                    parent_text = parent.get_text()
                    date_match = re.search(r'\d{4}-\d{2}-\d{2}', parent_text)
                    if date_match:
                        date_text = date_match.group()
            
            full_url = href if href.startswith('http') else self.BASE_URL + href
            
            articles.append({
                'title': title,
                'url': full_url,
                'date': date_text,
                'channel': channel_name,
                'is_military': is_military
            })
        
        return articles
    
    def fetch_article_content(self, url: str) -> Optional[str]:
        """å–å¾—å…¬å‘Šè©³ç´°å…§å®¹ä¸¦æ¸…ç†"""
        html = self.fetch_page(url)
        if not html:
            return None
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')
        
        # å˜—è©¦å¤šç¨®å…§å®¹é¸æ“‡å™¨
        content = None
        for selector in ['div.article-content', 'div.content', 'div.TRS_Editor',
                        'div.detail-content', 'article', 'div.main-content']:
            content_div = soup.select_one(selector)
            if content_div:
                content = content_div.get_text(strip=True)
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå–å¾— body å…§çš„ä¸»è¦æ–‡å­—
        if not content:
            body = soup.find('body')
            if body:
                for tag in body.find_all(['script', 'style', 'nav', 'header', 'footer']):
                    tag.decompose()
                content = body.get_text(separator=' ', strip=True)
        
        # æå–æ ¸å¿ƒå…§å®¹
        if content:
            content = self.extract_core_content(content)
        
        return content
    
    def extract_core_content(self, text: str) -> str:
        """æå–æ ¸å¿ƒå…§å®¹ï¼šå¾èˆªè­¦ç·¨è™Ÿåˆ°ã€Œæ”¶è—ã€ä¹‹é–“çš„æ–‡å­—"""
        # å°‹æ‰¾èˆªè­¦ç·¨è™Ÿé–‹å§‹ä½ç½®
        start_patterns = [
            r'([a-zA-Zæ²ªæ´¥è¾½å†€é²æµ™é—½ç²¤æ¡‚ç¼æ·±å¦ç”¬é’è¿ç æ±•æ¹›è‹]èˆªè­¦?\d+/\d+)',
            r'([A-Z]{2,3}\d+/\d+)',
        ]
        
        start_pos = -1
        
        for pattern in start_patterns:
            matches = list(re.finditer(pattern, text))
            if matches:
                for match in matches:
                    after_text = text[match.end():match.end()+50]
                    if 'ï¼Œ' in after_text or 'ã€‚' in after_text or ',' in after_text:
                        start_pos = match.start()
                        break
                if start_pos != -1:
                    break
        
        if start_pos == -1:
            return text[:500]
        
        # å°‹æ‰¾çµæŸä½ç½®
        end_patterns = ['æ”¶è—', 'æ‰“å°æœ¬é¡µ', 'å…³é—­çª—å£']
        end_pos = len(text)
        
        for end_pattern in end_patterns:
            pos = text.find(end_pattern, start_pos)
            if pos != -1 and pos < end_pos:
                end_pos = pos
        
        # æå–ä¸¦æ¸…ç†
        core_content = text[start_pos:end_pos].strip()
        core_content = re.sub(r'\s+', ' ', core_content)
        
        # é™åˆ¶é•·åº¦
        if len(core_content) > 1000:
            core_content = core_content[:1000] + '...'
        
        return core_content
    
    def parse_coordinates(self, text: str) -> List[Dict]:
        """è§£æç¶“ç·¯åº¦åº§æ¨™"""
        if not text:
            return []
        
        coords = []
        
        # æ ¼å¼1: 38-31.3N121-33.2E
        pattern1 = r'(\d{1,2})-(\d{1,2}(?:\.\d+)?)\s*([NS])\s*(\d{1,3})-(\d{1,2}(?:\.\d+)?)\s*([EW])'
        for match in re.finditer(pattern1, text):
            lat_deg, lat_min, lat_dir, lon_deg, lon_min, lon_dir = match.groups()
            lat = float(lat_deg) + float(lat_min) / 60
            lon = float(lon_deg) + float(lon_min) / 60
            if lat_dir == 'S':
                lat = -lat
            if lon_dir == 'W':
                lon = -lon
            coords.append({
                'lat': round(lat, 4),
                'lon': round(lon, 4),
                'raw': match.group()
            })
        
        # æ ¼å¼2: N 39Â°24â€²35â€³ã€E 119Â°13â€²44â€³
        pattern2 = r'([NS])?\s*(\d{1,2})Â°(\d{1,2})â€²(\d{1,2}(?:\.\d+)?)?â€³?\s*([NS])?\s*[ã€,\s]*([EW])?\s*(\d{1,3})Â°(\d{1,2})â€²(\d{1,2}(?:\.\d+)?)?â€³?\s*([EW])?'
        for match in re.finditer(pattern2, text):
            groups = match.groups()
            lat_dir = groups[0] or groups[4] or 'N'
            lat_deg, lat_min, lat_sec = groups[1], groups[2], groups[3] or '0'
            lon_dir = groups[5] or groups[9] or 'E'
            lon_deg, lon_min, lon_sec = groups[6], groups[7], groups[8] or '0'
            
            lat = float(lat_deg) + float(lat_min) / 60 + float(lat_sec) / 3600
            lon = float(lon_deg) + float(lon_min) / 60 + float(lon_sec) / 3600
            
            if lat_dir == 'S':
                lat = -lat
            if lon_dir == 'W':
                lon = -lon
            
            coords.append({
                'lat': round(lat, 4),
                'lon': round(lon, 4),
                'raw': match.group()
            })
        
        # æ ¼å¼3: 38.5N 121.5E
        pattern3 = r'(\d{1,2}(?:\.\d+)?)\s*([NS])\s*(\d{1,3}(?:\.\d+)?)\s*([EW])'
        for match in re.finditer(pattern3, text):
            lat, lat_dir, lon, lon_dir = match.groups()
            lat = float(lat)
            lon = float(lon)
            if lat_dir == 'S':
                lat = -lat
            if lon_dir == 'W':
                lon = -lon
            coords.append({
                'lat': round(lat, 4),
                'lon': round(lon, 4),
                'raw': match.group()
            })
        
        # å»é‡
        seen = set()
        unique_coords = []
        for c in coords:
            key = (c['lat'], c['lon'])
            if key not in seen:
                seen.add(key)
                unique_coords.append(c)
        
        return unique_coords
    
    def parse_time_period(self, text: str) -> List[str]:
        """è§£ææ™‚é–“ç¯„åœ"""
        if not text:
            return []
        
        times = []
        
        # æ ¼å¼1: XæœˆXæ—¥Xæ—¶è‡³Xæ—¥Xæ—¶
        pattern1 = r'(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2})æ—¶è‡³(\d{1,2})æ—¥?(\d{1,2})æ—¶'
        times.extend([m.group() for m in re.finditer(pattern1, text)])
        
        # æ ¼å¼2: è‡ªXæœˆXæ—¥Xæ—¶è‡³XæœˆXæ—¥Xæ—¶
        pattern2 = r'è‡ª?(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,4})æ—¶è‡³(\d{1,2})æœˆ?(\d{1,2})æ—¥?(\d{1,4})æ—¶'
        times.extend([m.group() for m in re.finditer(pattern2, text)])
        
        # æ ¼å¼3: XXXXå¹´XæœˆXæ—¥
        pattern3 = r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(?:\s*(\d{1,2}):?(\d{2}))?(?:æ—¶)?(?:è‡³|[-~])(\d{4})?å¹´?(\d{1,2})?æœˆ?(\d{1,2})æ—¥?(?:\s*(\d{1,2}):?(\d{2}))?(?:æ—¶)?'
        times.extend([m.group() for m in re.finditer(pattern3, text)])
        
        # æ ¼å¼4: Xæ—¥XXXXæ—¶è‡³XXXXæ—¶
        pattern4 = r'(\d{1,2})æ—¥(\d{4})æ—¶è‡³(\d{4})æ—¶'
        times.extend([m.group() for m in re.finditer(pattern4, text)])
        
        return list(set(times))
    
    def run(self, days_back: int = 365, max_pages: int = 1) -> List[Dict]:
        """
        åŸ·è¡Œçˆ¬èŸ² (ç¬¦åˆ BaseScraper çš„ç°½å)
        
        Args:
            days_back: è¿½æº¯å¤©æ•¸
            max_pages: æ¯å€‹æµ·äº‹å±€æœ€å¤šæŠ“å–é æ•¸ï¼ˆé è¨­1é =20ç¯‡ï¼‰
            
        Returns:
            æ¨™æº–æ ¼å¼çš„èˆªè¡Œè­¦å‘Šåˆ—è¡¨
        """
        print(f"[{self.name}] ğŸš¢ é–‹å§‹çˆ¬å– {len(self.CHANNELS)} å€‹æµ·äº‹å±€çš„èˆªè¡Œè­¦å‘Š...")
        
        all_warnings = []
        military_only = True  # å›ºå®šåªæŠ“è»äº‹ç›¸é—œ
        max_articles_per_channel = max_pages * 20  # æ¯é ç´„20ç¯‡
        
        for channel_name, channel_id in self.CHANNELS.items():
            print(f"[{self.name}] ğŸ“ æ­£åœ¨è™•ç†: {channel_name}")
            
            # å–å¾—åˆ—è¡¨
            articles = self.fetch_channel_list(channel_id, channel_name)
            
            if military_only:
                articles = [a for a in articles if a['is_military']]
            
            # æ—¥æœŸéæ¿¾
            filtered_articles = []
            for article in articles[:max_articles_per_channel]:
                if article['date']:
                    date_obj = self.parse_date(article['date'])
                    if date_obj and self.is_within_days(date_obj, days_back):
                        filtered_articles.append(article)
                else:
                    filtered_articles.append(article)
            
            articles = filtered_articles
            
            print(f"[{self.name}]    æ‰¾åˆ° {len(articles)} ç¯‡è»äº‹ç›¸é—œå…¬å‘Š")
            
            for article in articles:
                # å–å¾—è©³ç´°å…§å®¹
                content = self.fetch_article_content(article['url'])
                
                if content:
                    # è§£æåº§æ¨™
                    coordinates = self.parse_coordinates(content)
                    
                    # è§£ææ™‚é–“
                    time_periods = self.parse_time_period(content)
                    
                    warning = {
                        'title': article['title'],
                        'channel': channel_name,
                        'publish_date': article['date'],
                        'url': article['url'],
                        'coordinates': coordinates,
                        'coordinate_count': len(coordinates),
                        'time_periods': time_periods,
                        'content_preview': content,
                        'is_military': article['is_military'],
                        'scraped_at': datetime.now().isoformat()
                    }
                    
                    all_warnings.append(warning)
                
                import time
                time.sleep(0.5)
            
            import time
            time.sleep(1)
        
        print(f"[{self.name}] âœ… å®Œæˆï¼å…±æŠ“å– {len(all_warnings)} ç¯‡èˆªè¡Œè­¦å‘Š")
        
        return self.to_standard_format(all_warnings)
    
    def to_standard_format(self, warnings: List[Dict]) -> List[Dict]:
        """è½‰æ›ç‚ºæ¨™æº–æ ¼å¼"""
        standardized = []
        
        for w in warnings:
            # æ ¼å¼åŒ–ç¶“ç·¯åº¦
            coords_str = ''
            coords_raw = ''
            if w['coordinates']:
                coords_list = [f"{c['lat']},{c['lon']}" for c in w['coordinates']]
                coords_str = '; '.join(coords_list)
                coords_raw_list = [c['raw'] for c in w['coordinates']]
                coords_raw = '; '.join(coords_raw_list)
            
            # æ ¼å¼åŒ–æ™‚é–“ç¯„åœ
            time_periods_str = '; '.join(w['time_periods']) if w['time_periods'] else ''
            
            std_warning = {
                'publish_date': w['publish_date'],
                'title': w['title'],
                'channel': w['channel'],
                'time_periods': time_periods_str,
                'coordinate_count': w['coordinate_count'],
                'coordinates': coords_str,
                'coordinates_raw': coords_raw,
                'content_preview': w['content_preview'],
                'url': w['url'],
                'scraped_at': w['scraped_at']
            }
            
            standardized.append(std_warning)
        
        return standardized
