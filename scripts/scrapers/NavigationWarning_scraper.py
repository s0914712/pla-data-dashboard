#!/usr/bin/env python3
"""
===============================================================================
ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Šçˆ¬èŸ² (è»æ¼”å°ˆç”¨ç‰ˆ) / MSA Navigation Warning Scraper (Military Focus)
===============================================================================

ç›®æ¨™: https://www.msa.gov.cn/page/channelArticles.do?channelids=9C219298-B27F-460E-995A-99401B3FF6AF
ç”¨é€”: çˆ¬å–ä¸­åœ‹æµ·äº‹å±€ç™¼å¸ƒçš„è»äº‹æ¼”ç¿’ç›¸é—œèˆªè¡Œè­¦å‘Š
ä½œè€…: s0914712
GitHub: https://github.com/s0914712/pla-data-dashboard
"""

import time
import re
from datetime import datetime
from typing import List, Dict, Optional
from .base_scraper import BaseScraper


class NavigationWarningScraper(BaseScraper):
    """ä¸­åœ‹æµ·äº‹å±€èˆªè¡Œè­¦å‘Šçˆ¬èŸ²ï¼ˆè»æ¼”å°ˆç”¨ï¼‰"""
    
    BASE_URL = "https://www.msa.gov.cn"
    # èˆªè¡Œè­¦å‘Šç¸½é »é“ï¼ˆåŒ…å«æ‰€æœ‰æµ·äº‹å±€ï¼‰
    NAV_WARNING_CHANNEL = '9C219298-B27F-460E-995A-99401B3FF6AF'
    
    # ğŸ¯ è»äº‹æ¼”ç¿’é—œéµå­—ï¼ˆå¿…é ˆåŒ…å«è‡³å°‘ä¸€å€‹ï¼‰
    MILITARY_KEYWORDS = [
        # æ ¸å¿ƒè»äº‹é—œéµå­—
        'å†›äº‹', 'è»äº‹',
        'æ¼”ä¹ ', 'æ¼”ç¿’',
        'å®å¼¹', 'å¯¦å½ˆ',
        'ç«ç‚®å°„å‡»', 'ç«ç‚®å°„æ“Š',
        'å°„å‡»è®­ç»ƒ', 'å°„æ“Šè¨“ç·´',
        'ç¦èˆª', 'ç¦æ­¢é©¶å…¥', 'ç¦æ­¢é§›å…¥',
        'ç«ç®­å‘å°„', 'ç«ç®­ç™¼å°„',
        'ç«ç®­æ®‹éª¸', 'ç«ç®­æ®˜éª¸',
        'MILITARY', 'EXERCISE',
        
        # æ“´å±•é—œéµå­—
        'å†›æ¼”', 'è»æ¼”',
        'æ¼”è®­', 'æ¼”è¨“',
        'è”åˆæ¼”ç»ƒ', 'è¯åˆæ¼”ç·´',
        'å®æˆ˜åŒ–è®­ç»ƒ', 'å¯¦æˆ°åŒ–è¨“ç·´',
        'å¯¼å¼¹è¯•å°„', 'å°å½ˆè©¦å°„',
        'æ­¦å™¨è¯•éªŒ', 'æ­¦å™¨è©¦é©—',
        'æµ·ä¸Šå®å¼¹', 'æµ·ä¸Šå¯¦å½ˆ',
        'ç©ºä¸­æ¼”ç»ƒ', 'ç©ºä¸­æ¼”ç·´',
        'æˆ˜å¤‡å·¡èˆª', 'æˆ°å‚™å·¡èˆª',
        'å†›äº‹ç¦åŒº', 'è»äº‹ç¦å€',
        'é¶åœº', 'é¶å ´',
        'å°„å‡»åœº', 'å°„æ“Šå ´',
    ]
    
    # æ’é™¤é—œéµå­—ï¼ˆåŒ…å«é€™äº›çš„ä¸ç®—è»æ¼”ï¼‰
    EXCLUDE_KEYWORDS = [
        'æ‹–å¸¦', 'æ‹–å¸¶',
        'LNG', 'æ¶²åŒ–å¤©ç„¶æ°”',
        'æ–½å·¥', 'æµ·ä¸Šæ–½å·¥',
        'æ¸¬é‡', 'æµ‹é‡',
        'æ‰“æ', 'æ‰“æ’ˆ',
        'è½½è¿', 'è¼‰é‹',
        'å¤§ä»¶', 'è¶…å¤§ä»¶',
        'åŠ æ³¨', 'è£œçµ¦',
    ]
    
    # æµ·äº‹å±€ä»£ç¢¼æ˜ å°„
    MSA_CODE_MAP = {
        'æ²ª': 'ä¸Šæµ·æµ·äº‹å±€',
        'æ´¥': 'å¤©æ´¥æµ·äº‹å±€',
        'è¾½': 'è¾½å®æµ·äº‹å±€',
        'å†€': 'æ²³åŒ—æµ·äº‹å±€',
        'é²': 'å±±ä¸œæµ·äº‹å±€',
        'æµ™': 'æµ™æ±Ÿæµ·äº‹å±€',
        'é—½': 'ç¦å»ºæµ·äº‹å±€',
        'ç²¤': 'å¹¿ä¸œæµ·äº‹å±€',
        'æ¡‚': 'å¹¿è¥¿æµ·äº‹å±€',
        'ç¼': 'æµ·å—æµ·äº‹å±€',
        'æ·±': 'æ·±åœ³æµ·äº‹å±€',
        'å¦': 'å¦é—¨æµ·äº‹å±€',
        'ç”¬': 'å®æ³¢æµ·äº‹å±€',
        'é’': 'é’å²›æµ·äº‹å±€',
        'è¿': 'å¤§è¿æµ·äº‹å±€',
        'ç ': 'ç æµ·æµ·äº‹å±€',
        'æ±•': 'æ±•å¤´æµ·äº‹å±€',
        'æ¹›': 'æ¹›æ±Ÿæµ·äº‹å±€',
        'è‹': 'æ±Ÿè‹æµ·äº‹å±€',
        'é•¿æ±Ÿ': 'é•¿æ±Ÿæµ·äº‹å±€',
    }
    
    def __init__(self, timeout: int = 30, delay: float = 1.0):
        super().__init__(name="msa_military", timeout=timeout, delay=delay)
    
    def is_military_exercise(self, title: str) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºè»äº‹æ¼”ç¿’ç›¸é—œè­¦å‘Š
        
        Args:
            title: æ¨™é¡Œ
            
        Returns:
            æ˜¯å¦ç‚ºè»æ¼”ç›¸é—œ
        """
        title_lower = title.lower()
        
        # å…ˆæª¢æŸ¥æ’é™¤é—œéµå­—
        for exclude in self.EXCLUDE_KEYWORDS:
            if exclude.lower() in title_lower:
                return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«è»äº‹é—œéµå­—
        for keyword in self.MILITARY_KEYWORDS:
            if keyword.lower() in title_lower:
                return True
        
        return False
    
    def extract_msa_from_title(self, title: str) -> str:
        """å¾æ¨™é¡Œä¸­æå–æµ·äº‹å±€åç¨±"""
        # æ–¹æ³•1: åŒ¹é…èˆªè­¦ä»£ç¢¼ï¼ˆå¦‚ï¼šæ²ªèˆªè­¦88/26ï¼‰
        for code, msa_name in self.MSA_CODE_MAP.items():
            if f'{code}èˆªè­¦' in title or f'{code}èˆªè¡Œè­¦å‘Š' in title:
                return msa_name
        
        # æ–¹æ³•2: ç›´æ¥åŒ¹é…æµ·äº‹å±€åç¨±
        for msa_name in self.MSA_CODE_MAP.values():
            if msa_name.replace('æµ·äº‹å±€', '') in title:
                return msa_name
        
        return 'æœªçŸ¥æµ·äº‹å±€'
    
    def extract_matched_keywords(self, title: str) -> List[str]:
        """æå–åŒ¹é…åˆ°çš„è»äº‹é—œéµå­—"""
        matched = []
        title_lower = title.lower()
        
        for keyword in self.MILITARY_KEYWORDS:
            if keyword.lower() in title_lower:
                matched.append(keyword)
        
        return matched
    
    def scrape_page(self, page: int, page_size: int = 50) -> List[Dict]:
        """
        çˆ¬å–å–®é èˆªè¡Œè­¦å‘Šï¼ˆåªè¿”å›è»æ¼”ç›¸é—œï¼‰
        
        Args:
            page: é ç¢¼ï¼ˆå¾1é–‹å§‹ï¼‰
            page_size: æ¯é æ•¸é‡
            
        Returns:
            è»æ¼”ç›¸é—œèˆªè¡Œè­¦å‘Šåˆ—è¡¨
        """
        url = f"{self.BASE_URL}/page/channelArticles.do"
        params = {
            'channelids': self.NAV_WARNING_CHANNEL,
            'currpage': str(page),
            'pagesize': str(page_size)
        }
        
        html = self.fetch_page(
            url + '?' + '&'.join(f"{k}={v}" for k, v in params.items())
        )
        
        if not html:
            return []
        
        warnings = []
        
        # è§£æ HTML
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(html, 'html.parser')
        all_lis = soup.find_all('li')
        
        for li in all_lis:
            link = li.find('a', href=lambda x: x and 'articleId' in x)
            if not link:
                continue
            
            # æå–æ¨™é¡Œ
            title_span = link.find('span')
            title = title_span.text.strip() if title_span else link.text.strip()
            
            # ğŸ¯ é—œéµéæ¿¾ï¼šåªä¿ç•™è»æ¼”ç›¸é—œ
            if not self.is_military_exercise(title):
                continue
            
            # æå–æ—¥æœŸ
            date_text = None
            for span in li.find_all('span'):
                text = span.text.strip()
                if re.match(r'\[\d{4}-\d{2}-\d{2}\]', text):
                    date_text = text.strip('[]')
                    break
            
            # æå– article ID å’Œå®Œæ•´ URL
            href = link['href']
            article_id = None
            if 'articleId=' in href:
                article_id = href.split('articleId=')[1].split('&')[0]
            
            full_url = self.BASE_URL + href if not href.startswith('http') else href
            
            # è­˜åˆ¥æµ·äº‹å±€
            msa_name = self.extract_msa_from_title(title)
            
            # æå–åŒ¹é…çš„é—œéµå­—
            matched_keywords = self.extract_matched_keywords(title)
            
            warning = {
                'title': title,
                'msa': msa_name,
                'matched_keywords': ','.join(matched_keywords),
                'date': date_text,
                'article_id': article_id,
                'url': full_url,
                'scraped_at': datetime.now().isoformat()
            }
            
            warnings.append(warning)
        
        return warnings
    
    def run(self, max_pages: int = 50, days_back: int = 365) -> List[Dict]:
        """
        åŸ·è¡Œå®Œæ•´çˆ¬å–æµç¨‹
        
        Args:
            max_pages: æœ€å¤§çˆ¬å–é æ•¸ï¼ˆé è¨­50é ï¼Œç¢ºä¿è¦†è“‹è¶³å¤ ç¯„åœï¼‰
            days_back: çˆ¬å–éå»å¹¾å¤©çš„æ•¸æ“šï¼ˆé è¨­365å¤©ï¼Œä¸€å¹´å…§çš„è»æ¼”ï¼‰
            
        Returns:
            æ¨™æº–æ ¼å¼çš„è»æ¼”è­¦å‘Šåˆ—è¡¨
        """
        print(f"[{self.name}] ğŸ¯ é–‹å§‹çˆ¬å–è»äº‹æ¼”ç¿’ç›¸é—œèˆªè¡Œè­¦å‘Š...")
        print(f"[{self.name}] ğŸ“… ç›®æ¨™: éå» {days_back} å¤©ï¼Œæœ€å¤š {max_pages} é ")
        print(f"[{self.name}] ğŸ” è»äº‹é—œéµå­—: {len(self.MILITARY_KEYWORDS)} å€‹")
        
        all_warnings = []
        seen_ids = set()
        
        for page in range(1, max_pages + 1):
            print(f"[{self.name}] ğŸ“„ çˆ¬å–ç¬¬ {page}/{max_pages} é ...")
            
            warnings = self.scrape_page(page)
            
            if not warnings:
                print(f"[{self.name}] âš ï¸  ç¬¬ {page} é ç„¡è»æ¼”ç›¸é—œæ•¸æ“š")
                # ç¹¼çºŒçˆ¬å–ï¼Œä¸è¦åœæ­¢ï¼ˆå¯èƒ½åªæ˜¯é€™é æ²’æœ‰ï¼‰
                if page >= 10:  # ä½†å¦‚æœé€£çºŒ10é éƒ½æ²’æœ‰ï¼Œå‰‡åœæ­¢
                    consecutive_empty = True
                    for check_page in range(max(1, page - 9), page + 1):
                        if check_page in [w.get('_page', 0) for w in all_warnings]:
                            consecutive_empty = False
                            break
                    if consecutive_empty:
                        print(f"[{self.name}] âš ï¸  é€£çºŒå¤šé ç„¡æ•¸æ“šï¼Œåœæ­¢çˆ¬å–")
                        break
                continue
            
            # å»é‡ä¸¦éæ¿¾æ—¥æœŸ
            page_added = 0
            for warning in warnings:
                # æª¢æŸ¥é‡è¤‡
                if warning['article_id'] in seen_ids:
                    continue
                
                # æª¢æŸ¥æ—¥æœŸç¯„åœ
                date_obj = self.parse_date(warning['date'])
                if not date_obj or not self.is_within_days(date_obj, days_back):
                    continue
                
                seen_ids.add(warning['article_id'])
                warning['_page'] = page  # è¨˜éŒ„é ç¢¼ï¼ˆå…§éƒ¨ä½¿ç”¨ï¼‰
                all_warnings.append(warning)
                page_added += 1
            
            print(f"[{self.name}] âœ… æœ¬é æ–°å¢ {page_added} æ¢è»æ¼”è­¦å‘Šï¼Œç´¯è¨ˆ {len(all_warnings)} æ¢")
        
        print(f"\n[{self.name}] âœ… çˆ¬å–å®Œæˆï¼å…± {len(all_warnings)} æ¢è»äº‹æ¼”ç¿’è­¦å‘Š")
        
        # ç§»é™¤å…§éƒ¨å­—æ®µ
        for warning in all_warnings:
            warning.pop('_page', None)
        
        # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
        return self.to_standard_format(all_warnings)
    
    def to_standard_format(self, warnings: List[Dict]) -> List[Dict]:
        """
        è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
        
        æ¨™æº–æ ¼å¼:
        {
            'date': str (YYYY-MM-DD),
            'title': str,
            'msa': str (æµ·äº‹å±€åç¨±),
            'matched_keywords': str (åŒ¹é…çš„é—œéµå­—ï¼Œé€—è™Ÿåˆ†éš”),
            'article_id': str,
            'url': str,
            'scraped_at': str (ISOæ ¼å¼æ™‚é–“æˆ³)
        }
        """
        standardized = []
        for warning in warnings:
            date_obj = self.parse_date(warning.get('date', ''))
            std_warning = {
                'date': date_obj.strftime('%Y-%m-%d') if date_obj else '',
                'title': warning.get('title', '').strip(),
                'msa': warning.get('msa', ''),
                'matched_keywords': warning.get('matched_keywords', ''),
                'article_id': warning.get('article_id', ''),
                'url': warning.get('url', ''),
                'scraped_at': warning.get('scraped_at', '')
            }
            if std_warning['date'] and std_warning['title']:
                standardized.append(std_warning)
        
        return standardized


def test_scraper():
    """æ¸¬è©¦çˆ¬èŸ²"""
    print("=" * 80)
    print("MSA Military Exercise Warning Scraper æ¸¬è©¦")
    print("=" * 80)
    
    with NavigationWarningScraper(delay=1.0) as scraper:
        warnings = scraper.run(max_pages=10, days_back=180)
        
        print(f"\nç¸½è¨ˆçˆ¬å–: {len(warnings)} æ¢è»äº‹æ¼”ç¿’è­¦å‘Š\n")
        
        if not warnings:
            print("âš ï¸  æœªæ‰¾åˆ°è»äº‹æ¼”ç¿’ç›¸é—œè­¦å‘Š")
            return
        
        # æŒ‰æµ·äº‹å±€çµ±è¨ˆ
        from collections import Counter
        msa_counts = Counter(w['msa'] for w in warnings)
        
        print("æµ·äº‹å±€çµ±è¨ˆ:")
        for msa, count in msa_counts.most_common():
            print(f"  {msa}: {count} æ¢")
        
        # æŒ‰é—œéµå­—çµ±è¨ˆ
        all_keywords = []
        for w in warnings:
            if w['matched_keywords']:
                all_keywords.extend(w['matched_keywords'].split(','))
        
        keyword_counts = Counter(all_keywords)
        print("\né—œéµå­—çµ±è¨ˆ (Top 10):")
        for keyword, count in keyword_counts.most_common(10):
            print(f"  {keyword}: {count} æ¬¡")
        
        print("\næœ€æ–°10æ¢è­¦å‘Š:")
        sorted_warnings = sorted(warnings, key=lambda x: x['date'], reverse=True)
        for i, warning in enumerate(sorted_warnings[:10], 1):
            print(f"\n{i}. [{warning['date']}] {warning['title']}")
            print(f"   æµ·äº‹å±€: {warning['msa']}")
            print(f"   é—œéµå­—: {warning['matched_keywords']}")
            print(f"   URL: {warning['url']}")


if __name__ == '__main__':
    test_scraper()
