#!/usr/bin/env python3
"""
===============================================================================
æ–°è¯ç¤¾ç¨ç«‹çˆ¬å–è…³æœ¬ / Standalone Xinhua Crawl Script
===============================================================================

ç¨ç«‹åŸ·è¡Œæ–°è¯ç¤¾çˆ¬èŸ²ï¼Œå°‡çµæœç¶“ Grok åˆ†é¡å¾Œ **åˆä½µ** åˆ°ï¼š
  - data/news_classified.json
  - data/news_relevant.json

ç”¨æ³•:
  python scripts/crawl_xinhua.py [--days 7] [--no-push]

ç’°å¢ƒè®Šæ•¸:
  GROK_API_KEY â€” Grok åˆ†é¡å™¨é‡‘é‘°ï¼ˆå¿…è¦ï¼‰
"""

import argparse
import json
import sys
import os
from pathlib import Path
from datetime import datetime

# è·¯å¾‘è¨­å®š
sys.path.insert(0, str(Path(__file__).parent))

from scrapers.xinhua_scraper import XinhuaTWScraper
from classifiers.grok_classifier import GrokNewsClassifier


def load_existing_json(filepath: Path) -> list:
    """è®€å–æ—¢æœ‰ JSON æª”æ¡ˆï¼Œè‹¥ä¸å­˜åœ¨å‰‡å›å‚³ç©ºåˆ—è¡¨"""
    if filepath.exists():
        with filepath.open("r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if isinstance(data, list):
                    return data
            except json.JSONDecodeError:
                pass
    return []


def _get_article_url(article: dict) -> str:
    """å¾ classified çµæ§‹æˆ–åŸå§‹çµæ§‹ä¸­å–å¾— URLï¼ˆæ”¯æ´å·¢ç‹€ original_articleï¼‰"""
    return (
        article.get("url")
        or (article.get("original_article") or {}).get("url", "")
        or ""
    )


def merge_articles(existing: list, new_articles: list) -> list:
    """åˆä½µæ–‡ç« ï¼Œä»¥ original_article.url å»é‡"""
    seen_urls = {_get_article_url(a) for a in existing if _get_article_url(a)}
    merged = list(existing)
    added = 0
    for article in new_articles:
        url = _get_article_url(article)
        if url and url not in seen_urls:
            merged.append(article)
            seen_urls.add(url)
            added += 1
    print(f"  Merged: {added} new, {len(existing)} existing â†’ {len(merged)} total")
    return merged


def main():
    parser = argparse.ArgumentParser(description="Standalone Xinhua Scraper")
    parser.add_argument("--days", type=int, default=7, help="Days back to scrape")
    parser.add_argument("--no-push", action="store_true", help="Skip GitHub push")
    args = parser.parse_args()

    print("=" * 60)
    print(f"Xinhua Scraper â€” {datetime.now():%Y-%m-%d %H:%M:%S}")
    print("=" * 60)

    # ------------------------------------------------------------------
    # 1. çˆ¬å–æ–°è¯ç¤¾
    # ------------------------------------------------------------------
    print("\n[1/3] çˆ¬å–æ–°è¯ç¤¾æ–°è...")

    with XinhuaTWScraper() as scraper:
        articles = scraper.run(days_back=args.days)

    if not articles:
        print("âš ï¸  No articles scraped. Exiting.")
        sys.exit(0)

    print(f"âœ“ Scraped {len(articles)} articles")

    # ------------------------------------------------------------------
    # 2. Grok å»é‡ + åˆ†é¡
    # ------------------------------------------------------------------
    print("\n[2/3] Grok å»é‡ + åˆ†é¡...")

    api_key = os.environ.get("GROK_API_KEY")
    if not api_key:
        print("âŒ GROK_API_KEY not set. Exiting.")
        sys.exit(1)

    with GrokNewsClassifier(api_key) as classifier:
        deduped = classifier.deduplicate_batch(articles)
        classified = classifier.classify_batch(deduped, delay=1.0)

    # æ–°è¯ç¤¾ä¾†æºå¼·åˆ¶å¥—ç”¨åˆ†é¡è¦å‰‡ï¼ˆé‚è¼¯å°è£åœ¨ XinhuaTWScraperï¼‰
    classified = XinhuaTWScraper.apply_category_overrides(classified)
    relevant = [r for r in classified if r.get("is_relevant", False)]

    print(f"âœ“ Deduped: {len(articles)} â†’ {len(deduped)}")
    print(f"âœ“ Classified: {len(classified)}, Relevant: {len(relevant)}")

    # ------------------------------------------------------------------
    # 3. åˆä½µåˆ°æ—¢æœ‰ JSON
    # ------------------------------------------------------------------
    print("\n[3/3] åˆä½µåˆ°æ—¢æœ‰ JSON...")

    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    classified_file = data_dir / "news_classified.json"
    relevant_file = data_dir / "news_relevant.json"

    print(f"  {classified_file}:")
    merged_classified = merge_articles(load_existing_json(classified_file), classified)
    with classified_file.open("w", encoding="utf-8") as f:
        json.dump(merged_classified, f, ensure_ascii=False, indent=2)

    print(f"  {relevant_file}:")
    merged_relevant = merge_articles(load_existing_json(relevant_file), relevant)
    with relevant_file.open("w", encoding="utf-8") as f:
        json.dump(merged_relevant, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ Saved: {classified_file}")
    print(f"âœ“ Saved: {relevant_file}")

    # ------------------------------------------------------------------
    # 4. Git push
    # ------------------------------------------------------------------
    if not args.no_push:
        from updaters.github_updater import GitHubUpdater
        try:
            updater = GitHubUpdater()
            updater.configure_git(name="PLA Data Bot", email="bot@example.com")
            success = updater.commit_and_push_data(
                data_files=[str(classified_file), str(relevant_file)],
                message=f"ğŸ¤– Xinhua update: {datetime.now():%Y-%m-%d %H:%M}",
            )
            print("âœ“ Pushed to GitHub" if success else "âš ï¸  No changes to push")
        except Exception as e:
            print(f"âœ— GitHub push error: {e}")

    print("\n" + "=" * 60)
    print("âœ… Xinhua scrape completed!")
    print("=" * 60)


if __name__ == "__main__":
    main()
