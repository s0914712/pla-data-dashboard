#!/usr/bin/env python3
"""
===============================================================================
å¾®åšç¨ç«‹çˆ¬å–è…³æœ¬ / Standalone Weibo Crawl Script
===============================================================================

ç¨ç«‹åŸ·è¡Œå¾®åšçˆ¬èŸ²ï¼Œå°‡çµæœç¶“ Grok åˆ†é¡å¾Œ **åˆä½µ** åˆ°ï¼š
  - data/news_classified.json
  - data/news_relevant.json

ç”¨æ³•:
  python scripts/crawl_weibo.py [--days 7] [--no-push]

ç’°å¢ƒè®Šæ•¸:
  WEIBO_COOKIE   â€” å¾®åš Mobile ç«¯ Cookieï¼ˆå¿…è¦ï¼‰
  GROK_API_KEY   â€” Grok åˆ†é¡å™¨é‡‘é‘°ï¼ˆå¿…è¦ï¼‰
  WEIBO_TARGET_UID â€” ç›®æ¨™ UIDï¼ˆå¯é¸ï¼Œé è¨­æ±éƒ¨æˆ°å€ï¼‰
"""

import argparse
import json
import sys
import os
from pathlib import Path
from datetime import datetime

# è·¯å¾‘è¨­å®š
sys.path.insert(0, str(Path(__file__).parent))

from scrapers.weibo_scraper import WeiboScraper
from classifiers.grok_classifier import GrokNewsClassifier


def load_existing_json(filepath: Path) -> list:
    """è®€å–æ—¢æœ‰ JSON æª”æ¡ˆï¼Œè‹¥ä¸å­˜åœ¨å‰‡å›å‚³ç©ºåˆ—è¡¨"""
    if filepath.exists():
        with filepath.open("r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if isinstance(data, list):
                    return data
            except json.JSONDecodeError:
                pass
    return []


def merge_articles(existing: list, new_articles: list) -> list:
    """
    åˆä½µæ–‡ç« ï¼Œä»¥ url å»é‡ã€‚
    æ–°æ–‡ç« è‹¥ url å·²å­˜åœ¨å‰‡è·³éï¼Œå¦å‰‡é™„åŠ ã€‚
    """
    seen_urls = {a.get("url") for a in existing if a.get("url")}
    merged = list(existing)
    added = 0
    for article in new_articles:
        url = article.get("url", "")
        if url and url not in seen_urls:
            merged.append(article)
            seen_urls.add(url)
            added += 1
    print(f"  Merged: {added} new, {len(existing)} existing â†’ {len(merged)} total")
    return merged


def main():
    parser = argparse.ArgumentParser(description="Standalone Weibo Scraper")
    parser.add_argument("--days", type=int, default=7, help="Days back to scrape")
    parser.add_argument("--max-pages", type=int, default=5, help="Max pages to scrape")
    parser.add_argument("--no-push", action="store_true", help="Skip GitHub push")
    args = parser.parse_args()

    print("=" * 60)
    print(f"Weibo Scraper â€” {datetime.now():%Y-%m-%d %H:%M:%S}")
    print("=" * 60)

    # ------------------------------------------------------------------
    # 1. çˆ¬å–å¾®åš
    # ------------------------------------------------------------------
    print("\n[1/3] çˆ¬å–å¾®åšè²¼æ–‡...")

    if not os.environ.get("WEIBO_COOKIE"):
        print("âŒ WEIBO_COOKIE not set. Exiting.")
        sys.exit(1)

    with WeiboScraper(max_pages=args.max_pages) as scraper:
        articles = scraper.run(days_back=args.days)

    if not articles:
        print("âš ï¸  No articles scraped. Exiting.")
        sys.exit(0)

    print(f"âœ“ Scraped {len(articles)} posts")

    # ------------------------------------------------------------------
    # 2. Grok å»é‡ + åˆ†é¡
    # ------------------------------------------------------------------
    print("\n[2/3] Grok å»é‡ + åˆ†é¡...")

    api_key = os.environ.get("GROK_API_KEY")
    if not api_key:
        print("âŒ GROK_API_KEY not set. Exiting.")
        sys.exit(1)

    with GrokNewsClassifier(api_key) as classifier:
        deduped = classifier.deduplicate_batch(articles)
        classified = classifier.classify_batch(deduped, delay=1.0)
        relevant = classifier.filter_relevant(classified)

    print(f"âœ“ Deduped: {len(articles)} â†’ {len(deduped)}")
    print(f"âœ“ Classified: {len(classified)}, Relevant: {len(relevant)}")

    # ------------------------------------------------------------------
    # 3. åˆä½µåˆ°æ—¢æœ‰ JSON
    # ------------------------------------------------------------------
    print("\n[3/3] åˆä½µåˆ°æ—¢æœ‰ JSON...")

    data_dir = Path("data")
    data_dir.mkdir(exist_ok=True)

    classified_file = data_dir / "news_classified.json"
    relevant_file = data_dir / "news_relevant.json"

    # è®€å– â†’ åˆä½µ â†’ å¯«å›
    print(f"  {classified_file}:")
    merged_classified = merge_articles(load_existing_json(classified_file), classified)
    with classified_file.open("w", encoding="utf-8") as f:
        json.dump(merged_classified, f, ensure_ascii=False, indent=2)

    print(f"  {relevant_file}:")
    merged_relevant = merge_articles(load_existing_json(relevant_file), relevant)
    with relevant_file.open("w", encoding="utf-8") as f:
        json.dump(merged_relevant, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ Saved: {classified_file}")
    print(f"âœ“ Saved: {relevant_file}")

    # ------------------------------------------------------------------
    # 4. Git commit & push (ç”± Action è™•ç†æˆ–æ‰‹å‹•)
    # ------------------------------------------------------------------
    if not args.no_push:
        from updaters.github_updater import GitHubUpdater
        try:
            updater = GitHubUpdater()
            updater.configure_git(name="PLA Data Bot", email="bot@example.com")
            success = updater.commit_and_push_data(
                data_files=[str(classified_file), str(relevant_file)],
                message=f"ğŸ¤– Weibo update: {datetime.now():%Y-%m-%d %H:%M}",
            )
            print("âœ“ Pushed to GitHub" if success else "âš ï¸  No changes to push")
        except Exception as e:
            print(f"âœ— GitHub push error: {e}")

    print("\n" + "=" * 60)
    print("âœ… Weibo scrape completed!")
    print("=" * 60)


if __name__ == "__main__":
    main()
